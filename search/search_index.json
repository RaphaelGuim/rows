{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to rows documentation! No matter in which format your tabular data is: rows will import it, automatically detect types and give you high-level Python objects so you can start working with the data instead of trying to parse it . It is also locale-and-unicode aware. :) Have you ever lost your precious time reading a CSV that had a different dialect? Or trying to learn a whole new library API to read a new tabular data format your customer just sent? You've got gray hair when trying to access some data and the only answer was UnicodeDecodeError ? So, rows was custom made for you - run pip install rows and be happy! :-) The library is officialy supported on Python versions 2.7, 3.5 and 3.6 (but may work on other versions too). Note: if you're using rows in some project please tell us ! :-) Contents First Steps Basic Usage Quick-start guide Installation License Basic Concepts Architecture Plugins Operations Command-line Locale Going deeper Contributing Code reference","title":"Welcome to rows documentation!"},{"location":"#welcome-to-rows-documentation","text":"No matter in which format your tabular data is: rows will import it, automatically detect types and give you high-level Python objects so you can start working with the data instead of trying to parse it . It is also locale-and-unicode aware. :) Have you ever lost your precious time reading a CSV that had a different dialect? Or trying to learn a whole new library API to read a new tabular data format your customer just sent? You've got gray hair when trying to access some data and the only answer was UnicodeDecodeError ? So, rows was custom made for you - run pip install rows and be happy! :-) The library is officialy supported on Python versions 2.7, 3.5 and 3.6 (but may work on other versions too). Note: if you're using rows in some project please tell us ! :-)","title":"Welcome to rows documentation!"},{"location":"#contents","text":"First Steps Basic Usage Quick-start guide Installation License Basic Concepts Architecture Plugins Operations Command-line Locale Going deeper Contributing Code reference","title":"Contents"},{"location":"architecture/","text":"Architecture The library is composed by: A common interface to tabular data (the Table class) A set of plugins to populate Table objects from formats like CSV, XLS, XLSX, HTML and XPath, Parquet, PDF, TXT, JSON, SQLite; A set of common fields (such as BoolField , IntegerField ) which know exactly how to serialize and deserialize data for each object type you'll get A set of utilities (such as field type recognition) to help working with tabular data A command-line interface so you can have easy access to the most used features: convert between formats, sum, join and sort tables.","title":"Architecture"},{"location":"architecture/#architecture","text":"The library is composed by: A common interface to tabular data (the Table class) A set of plugins to populate Table objects from formats like CSV, XLS, XLSX, HTML and XPath, Parquet, PDF, TXT, JSON, SQLite; A set of common fields (such as BoolField , IntegerField ) which know exactly how to serialize and deserialize data for each object type you'll get A set of utilities (such as field type recognition) to help working with tabular data A command-line interface so you can have easy access to the most used features: convert between formats, sum, join and sort tables.","title":"Architecture"},{"location":"basic-usage/","text":"Basic Usage rows will import tabular data in any of the supported formats, automatically detect/convert encoding and column types for you, so you can focus on work on the data. Given a CSV file like this: state,city,inhabitants,area AC,Acrel\u00e2ndia,12538,1807.92 AC,Assis Brasil,6072,4974.18 AC,Brasil\u00e9ia,21398,3916.5 AC,Bujari,8471,3034.87 AC,Capixaba,8798,1702.58 [...] RJ,Angra dos Reis,169511,825.09 RJ,Aperib\u00e9,10213,94.64 RJ,Araruama,112008,638.02 RJ,Areal,11423,110.92 RJ,Arma\u00e7\u00e3o dos B\u00fazios,27560,70.28 [...] You can use rows to do some math with it without the need to convert anything: import rows cities = rows.import_from_csv(\"data/brazilian-cities.csv\") rio_biggest_cities = [ city for city in cities if city.state == \"RJ\" and city.inhabitants > 500000 ] for city in rio_biggest_cities: density = city.inhabitants / city.area print(f\"{city.city} ({density:5.2f} ppl/km\u00b2)\") Note: download brazilian-cities.csv . The result: Duque de Caxias (1828.51 ppl/km\u00b2) Nova Igua\u00e7u (1527.59 ppl/km\u00b2) Rio de Janeiro (5265.81 ppl/km\u00b2) S\u00e3o Gon\u00e7alo (4035.88 ppl/km\u00b2) The library can also export data in any of the available plugins and have a command-line interface for more common tasks. For more examples, please refer to our quick-start guide . Note: rows is still not lazy by default, except for some operations like csv2sqlite , sqlite2csv , pgimport and pgexport (so using rows.import_from_X will put everything in memory), we're working on this .","title":"Basic Usage"},{"location":"basic-usage/#basic-usage","text":"rows will import tabular data in any of the supported formats, automatically detect/convert encoding and column types for you, so you can focus on work on the data. Given a CSV file like this: state,city,inhabitants,area AC,Acrel\u00e2ndia,12538,1807.92 AC,Assis Brasil,6072,4974.18 AC,Brasil\u00e9ia,21398,3916.5 AC,Bujari,8471,3034.87 AC,Capixaba,8798,1702.58 [...] RJ,Angra dos Reis,169511,825.09 RJ,Aperib\u00e9,10213,94.64 RJ,Araruama,112008,638.02 RJ,Areal,11423,110.92 RJ,Arma\u00e7\u00e3o dos B\u00fazios,27560,70.28 [...] You can use rows to do some math with it without the need to convert anything: import rows cities = rows.import_from_csv(\"data/brazilian-cities.csv\") rio_biggest_cities = [ city for city in cities if city.state == \"RJ\" and city.inhabitants > 500000 ] for city in rio_biggest_cities: density = city.inhabitants / city.area print(f\"{city.city} ({density:5.2f} ppl/km\u00b2)\") Note: download brazilian-cities.csv . The result: Duque de Caxias (1828.51 ppl/km\u00b2) Nova Igua\u00e7u (1527.59 ppl/km\u00b2) Rio de Janeiro (5265.81 ppl/km\u00b2) S\u00e3o Gon\u00e7alo (4035.88 ppl/km\u00b2) The library can also export data in any of the available plugins and have a command-line interface for more common tasks. For more examples, please refer to our quick-start guide . Note: rows is still not lazy by default, except for some operations like csv2sqlite , sqlite2csv , pgimport and pgexport (so using rows.import_from_X will put everything in memory), we're working on this .","title":"Basic Usage"},{"location":"changelog/","text":"Release notes Semantic Versioning rows uses [semantic versioning][semver]. Note that it means we do not guarantee API backwards compatibility on 0.x.y versions (but we try the best to). Version 0.4.2dev0 Released on: (in development) General Changes and Enhancements export_to_html is now available even if lxml is not installed Add Jupyter Notebook integration (implements _repr_html_ , .head and .tail ) Fix code to remove some warnings Add support to read compressed files directly (like in rows.import_from_csv(\"filename.csv.gz\") ) rows.Table now returns a new table when sliced Remove functions export_data and get_filename_and_fobj (the new Source implements the features better). Plugins Add param max_rows to create_table (import only part of a table, all plugins are supported) Add start_row , end_row , start_column and end_column to ODS plugin Prevent xlrd (XLS plugin) from printing wrong sector size warning (\" WARNING *** file size (551546) not 512 + multiple of sector size (512) \") Set rows.Table name ( table.meta[\"name\"] ) for ODS, XLS and XLSX plugins Add option to set <caption> tag in export_to_html Use correct table name when exporting to PostgreSQL Carefully close all fobjs in pgimport/pgexport Added CSV dialect \"excel-semicolon\" Improved PostgreSQL import from CSV (pgimport) when dealing with null values PDF now supports page_numbers as string (range of numbers) Add support to exporto to multiple sheets on the same XLSX file Command-Line Interface rows schema is now \"lazy\" (before it imported the whole file, even if samples were defined) Add support for compressed files output on rows pdf-to-text and rows schema HTTP cache is disabled by default (this may change in the future) Accept URI schemes in rows convert rows convert now supports compressed files rows pgexport now accepts query instead of table name (useful for selecting from a view since \\copy cannot use a view but can use a query instead of a table name). Detect input encoding whenever possible Add --quiet to some commands (fix progress ) Add plugins' input/output options to convert Add rows csv-merge (lazily merge CSV files even if they don't share a common schema) Add rows csv-clean (lazily clean a CSV file, removing empty columns and creating a consistent output format) Add rows list-sheets (prints sheet names for ODS, XLS and XLSX files) Utils Add support for CSV format on schema export Use dataclasses to describe Source import_from_source now supports compressed files (and so all CLI commands) Add support for passing a context to load_schema Bug Fixes #314 rows pgimport fails if using --schema #309 Fix file-magic detection #320 Get correct data if ODS spreadsheet has empty cells Fix slug function (so \"a/b\" will turn into \"a_b\" ) Detect as fallback type if all values are empty Fix output on rows schema (was printing to stdout even if output file is provided) Fix rows schema (some output formats where not working properly) Version 0.4.1 (bugfix release) Released on: 2019-02-14 General Changes and Enhancements Add new way to make docs (remove sphinx and uses mkdocs + click-man + pycco) Update Dockerfile Bug Fixes #305 \"0\" was not being deserialized by IntegerField Version 0.4.0 Released on: 2019-02-09 General Changes and Enhancements #243 Change license to LGPL3.0. Added official Python 3.6 support. Table.__add__ does not depend on table sizes anymore. Implemented Table.__iadd__ ( table += other will work). #234 Remove BinaryField from the default list of detection types. Plugins #224 Add | as possible delimiter (CSV dialect detection). Export CSV in batches. Change CSV dialect detection sample size to 256KiB. #225 Create export callbacks (CSV and SQLite plugins). #270 Added options to export pretty text table frames (TXT plugin). #274 start_row and start_column now behave the same way in XLS and XLSX (starting from 0). #261 Add support to end_row and end_column on XLS and XLSX (thanks @Lrcezimbra for the suggestion). #4 Add PostgreSQL plugin (thanks to @juliano777 ). #290 Fix percent formatting reading on XLSX and ODS file formats (thanks to @jsbueno ). #220 Do not use non-import_fields and force_types columns on type detection algorithm. #50 Create PDF extraction plugin with two backend libraries ( pymupdf and pdfminer.six ) and 3 table extraction algorithms. #294 Decrease XLSX reading time (thanks to @israelst ). Change to pure Python version of Apache Thrift library (parquet plugin) @299 Change CSV field limit Command-Line Interface #242 Add --fields / --fields-exclude to convert , join and sum (and rename --fields-exclude on print ), also remove --fields from query (is not needed). #235 Implement --http-cache and --http-cache-path . #237 Implement rows schema (generates schema in text, SQL and Django models). Enable progress bar when downloading files. Create pgimport and pgexport commands. Create csv-to-sqlite and sqlite-to-csv commands. Create pdf-to-text command. Add shortcut for all command names: 2 can be used instead of -to- (so rows pdf2text is a shortcut to rows pdf-to-text ). Utils Create utils.open_compressed helper function: can read/write files, automatically dealing with on-the-fly compression. Add progress bar support to utils.download_file (thanks to tqdm library). Add helper class utils.CsvLazyDictWriter (write as dict s without needing to pass the keys in advance). Add utils.pgimport and utils.pgexport functions. Add utils.csv2sqlite and utils.sqlite2csv functions. Bug Fixes #223 UnicodeDecodeError on dialect detection. #214 Problem detecting dialect. #181 Create slugs inside Table.__init__ . #221 Error on pip install rows . #238 import_from_dicts supports generator as input #239 Use correct field ordering #299 Integer field detected for numbers started with zero Version 0.3.1 Released on: 2017-05-08 Enhancements Move information on README to a site, organize and add more examples. Documentation is available at turicas.info/rows . Thanks to @ellisonleao for Sphinx implementation and @ramiroluz for new examples. Little code refactorings. Bug Fixes #200 Escape output when exporting to HTML (thanks to @arloc ) Fix some tests #215 DecimalField does not handle negative values correctly if using locale (thanks to @draug3n for reporting) Version 0.3.0 Released on: 2016-09-02 Backwards Incompatible Changes Bug Fixes Return None on XLS blank cells; #188 Change sample_size on encoding detection. Enhancements and Refactorings rows.fields.detect_fields will consider BinaryField if all the values are str (Python 2)/ bytes (Python 3) and all other fields will work only with unicode (Python 2)/ str (Python 3); Plugins HTML and XPath now uses a better way to return inner HTML (when preserve_html=True ); #189 Optimize Table.__add__ . New Features Support for Python 3 (finally!); rows.fields.BinaryField now automatically uses base64 to encode/decode; Added encoding information to rows.Table metadata in text plugins; Added sheet_name information to rows.Table metadata in XLS and XLSX plugins; #190 Add query_args to import_from_sqlite ; #177 Add dialect to export_to_csv . Version 0.2.1 Released on: 2016-08-10 Backwards Incompatible Changes rows.utils.export_to_uri signature is now like rows.export_to_* (first the rows.Table object, then the URI) Changed default table name in import_from_sqlite and export_to_sqlite (from rows and rows_{number} to table{number} ) Bug Fixes #170 (SQLite plugin) Error converting int and float when value is None . #168 Use Field.serialize if does not know the field type (affecting: XLS, XLSX and SQLite plugins). #167 Use more data to detect dialect, delimit the possible delimiters and fallback to excel if can't detect. #176 Problem using quotes on CSV plugin. #179 Fix double underscore problem on rows.utils.slug #175 Fix None serialization/deserialization in all plugins (and also field types) #172 Expose all tables in rows query for SQLite databases Fix examples/cli/convert.sh (missing - ) Avoids SQL injection in table name Enhancements and Refactorings Refactor rows.utils.import_from_uri Encoding and file type are better detected on rows.utils.import_from_uri Added helper functions to rows.utils regarding encoding and file type/plugin detection There's a better description of plugin metadata (MIME types accepted) on rows.utils (should be refactored to be inside each plugin) Moved slug and ipartition functions to rows.plugins.utils Optimize rows query when using only one SQLite source Version 0.2.0 Released on: 2016-07-15 Backwards Incompatible Changes rows.fields.UnicodeField was renamed to rows.fields.TextField rows.fields.BytesField was renamed to rows.fields.BinaryField Bug Fixes Fix import errors on older versions of urllib3 and Python (thanks to @jeanferri ) #156 BoolField should not accept \"0\" and \"1\" as possible values #86 Fix Content-Type parsing Fix locale-related tests #85 Fix preserve_html if fields is not provided Fix problem with big integers #131 Fix problem when empty sample data Fix problem with unicode and DateField Fix PercentField.serialize(None) Fix bug with Decimal receiving '' Fix bug in PercentField.serialize(Decimal('0')) Fix nested table behaviour on HTML plugin General Changes (EXPERIMENTAL) Add rows.FlexibleTable class (with help on tests from @maurobaraildi ) Lots of refactorings Add rows.operations.transpose Add Table.__repr__ Renamte rows.fields.UnicodeField to rows.fields.TextField and rows.fields.ByteField to rows.fields.BinaryField Add a man page (thanks to @kretcheu ) #40 The package is available on Debian! #120 The package is available on Fedora! Add some examples #138 Add rows.fields.JSONField #146 Add rows.fields.EmailField Enhance encoding detection using file-magic library #160 Add support for column get/set/del in rows.Table Tests Fix \"\\r\\n\" on tests to work on Windows Enhance tests with mock to assure some functions are being called Improve some tests Plugins Add plugin JSON (thanks @sxslex ) #107 Add import_from_txt #149 Add import_from_xpath (EXPERIMENTAL) Add import_from_ods (EXPERIMENTAL) Add import_from_parquet Add import_from_sqlite and export_to_sqlite (implemented by @turicas with help from @infog ) Add import_from_xlsx and export_to_xlsx (thanks to @RhenanBartels ) Autodetect delimiter in CSV files Export to TXT, JSON and XLS also support an already opened file and CSV can export to memory (thanks to @jeanferri ) #93 Add HTML helpers inside rows.plugins.html : count_tables , extract_text , extract_links and tag_to_dict #162 Add import_from_dicts and export_to_dicts Refactor export_to_txt Utils Create rows.plugins.utils #119 Rename field name if name is duplicated (to \"field_2\", \"field_3\", ..., \"field_N\") or if starts with a number. Add option to import only some fields ( import_fields parameter inside create_table ) Add option to export only some fields ( export_fields parameter inside prepare_to_export ) Add option force_types to force field types in some columns (instead of detecting) on create_table . Support lazy objects on create_table Add samples parameter to create_table CLI Add option to disable SSL verification ( --verify-ssl=no ) Add print command Add --version CLI is not installed by default (should be installed as pip install rows[cli] ) Automatically detect default encoding (if not specified) Add --order-by to some commands and remove sort command. #111 Do not use locale by default Add query command: converts (from many sources) internally to SQLite, execute the query and then export Version 0.1.1 Released on: 2015-09-03 Fix code to run on Windows (thanks @sxslex ) Fix locale (name, default name etc.) Remove filemagic dependency (waiting for python-magic to be available on PyPI) Write log of changes for 0.1.0 and 0.1.1 Version 0.1.0 Released on: 2015-08-29 Implement Table and its basic methods Implement basic plugin support with many utilities and the following formats: csv (input/output) html (input/output) txt (output) xls (input/output) Implement the following field types - many of them with locale support: ByteField BoolField IntegerField FloatField DecimalField PercentField DateField DatetimeField UnicodeField Implement basic Table operations: sum join transform serialize Implement a command-line interface with the following commands: convert join sort sum Add examples to the repository","title":"Release notes"},{"location":"changelog/#release-notes","text":"","title":"Release notes"},{"location":"changelog/#semantic-versioning","text":"rows uses [semantic versioning][semver]. Note that it means we do not guarantee API backwards compatibility on 0.x.y versions (but we try the best to).","title":"Semantic Versioning"},{"location":"changelog/#version-042dev0","text":"Released on: (in development)","title":"Version 0.4.2dev0"},{"location":"changelog/#general-changes-and-enhancements","text":"export_to_html is now available even if lxml is not installed Add Jupyter Notebook integration (implements _repr_html_ , .head and .tail ) Fix code to remove some warnings Add support to read compressed files directly (like in rows.import_from_csv(\"filename.csv.gz\") ) rows.Table now returns a new table when sliced Remove functions export_data and get_filename_and_fobj (the new Source implements the features better).","title":"General Changes and Enhancements"},{"location":"changelog/#plugins","text":"Add param max_rows to create_table (import only part of a table, all plugins are supported) Add start_row , end_row , start_column and end_column to ODS plugin Prevent xlrd (XLS plugin) from printing wrong sector size warning (\" WARNING *** file size (551546) not 512 + multiple of sector size (512) \") Set rows.Table name ( table.meta[\"name\"] ) for ODS, XLS and XLSX plugins Add option to set <caption> tag in export_to_html Use correct table name when exporting to PostgreSQL Carefully close all fobjs in pgimport/pgexport Added CSV dialect \"excel-semicolon\" Improved PostgreSQL import from CSV (pgimport) when dealing with null values PDF now supports page_numbers as string (range of numbers) Add support to exporto to multiple sheets on the same XLSX file","title":"Plugins"},{"location":"changelog/#command-line-interface","text":"rows schema is now \"lazy\" (before it imported the whole file, even if samples were defined) Add support for compressed files output on rows pdf-to-text and rows schema HTTP cache is disabled by default (this may change in the future) Accept URI schemes in rows convert rows convert now supports compressed files rows pgexport now accepts query instead of table name (useful for selecting from a view since \\copy cannot use a view but can use a query instead of a table name). Detect input encoding whenever possible Add --quiet to some commands (fix progress ) Add plugins' input/output options to convert Add rows csv-merge (lazily merge CSV files even if they don't share a common schema) Add rows csv-clean (lazily clean a CSV file, removing empty columns and creating a consistent output format) Add rows list-sheets (prints sheet names for ODS, XLS and XLSX files)","title":"Command-Line Interface"},{"location":"changelog/#utils","text":"Add support for CSV format on schema export Use dataclasses to describe Source import_from_source now supports compressed files (and so all CLI commands) Add support for passing a context to load_schema","title":"Utils"},{"location":"changelog/#bug-fixes","text":"#314 rows pgimport fails if using --schema #309 Fix file-magic detection #320 Get correct data if ODS spreadsheet has empty cells Fix slug function (so \"a/b\" will turn into \"a_b\" ) Detect as fallback type if all values are empty Fix output on rows schema (was printing to stdout even if output file is provided) Fix rows schema (some output formats where not working properly)","title":"Bug Fixes"},{"location":"changelog/#version-041-bugfix-release","text":"Released on: 2019-02-14","title":"Version 0.4.1 (bugfix release)"},{"location":"changelog/#general-changes-and-enhancements_1","text":"Add new way to make docs (remove sphinx and uses mkdocs + click-man + pycco) Update Dockerfile","title":"General Changes and Enhancements"},{"location":"changelog/#bug-fixes_1","text":"#305 \"0\" was not being deserialized by IntegerField","title":"Bug Fixes"},{"location":"changelog/#version-040","text":"Released on: 2019-02-09","title":"Version 0.4.0"},{"location":"changelog/#general-changes-and-enhancements_2","text":"#243 Change license to LGPL3.0. Added official Python 3.6 support. Table.__add__ does not depend on table sizes anymore. Implemented Table.__iadd__ ( table += other will work). #234 Remove BinaryField from the default list of detection types.","title":"General Changes and Enhancements"},{"location":"changelog/#plugins_1","text":"#224 Add | as possible delimiter (CSV dialect detection). Export CSV in batches. Change CSV dialect detection sample size to 256KiB. #225 Create export callbacks (CSV and SQLite plugins). #270 Added options to export pretty text table frames (TXT plugin). #274 start_row and start_column now behave the same way in XLS and XLSX (starting from 0). #261 Add support to end_row and end_column on XLS and XLSX (thanks @Lrcezimbra for the suggestion). #4 Add PostgreSQL plugin (thanks to @juliano777 ). #290 Fix percent formatting reading on XLSX and ODS file formats (thanks to @jsbueno ). #220 Do not use non-import_fields and force_types columns on type detection algorithm. #50 Create PDF extraction plugin with two backend libraries ( pymupdf and pdfminer.six ) and 3 table extraction algorithms. #294 Decrease XLSX reading time (thanks to @israelst ). Change to pure Python version of Apache Thrift library (parquet plugin) @299 Change CSV field limit","title":"Plugins"},{"location":"changelog/#command-line-interface_1","text":"#242 Add --fields / --fields-exclude to convert , join and sum (and rename --fields-exclude on print ), also remove --fields from query (is not needed). #235 Implement --http-cache and --http-cache-path . #237 Implement rows schema (generates schema in text, SQL and Django models). Enable progress bar when downloading files. Create pgimport and pgexport commands. Create csv-to-sqlite and sqlite-to-csv commands. Create pdf-to-text command. Add shortcut for all command names: 2 can be used instead of -to- (so rows pdf2text is a shortcut to rows pdf-to-text ).","title":"Command-Line Interface"},{"location":"changelog/#utils_1","text":"Create utils.open_compressed helper function: can read/write files, automatically dealing with on-the-fly compression. Add progress bar support to utils.download_file (thanks to tqdm library). Add helper class utils.CsvLazyDictWriter (write as dict s without needing to pass the keys in advance). Add utils.pgimport and utils.pgexport functions. Add utils.csv2sqlite and utils.sqlite2csv functions.","title":"Utils"},{"location":"changelog/#bug-fixes_2","text":"#223 UnicodeDecodeError on dialect detection. #214 Problem detecting dialect. #181 Create slugs inside Table.__init__ . #221 Error on pip install rows . #238 import_from_dicts supports generator as input #239 Use correct field ordering #299 Integer field detected for numbers started with zero","title":"Bug Fixes"},{"location":"changelog/#version-031","text":"Released on: 2017-05-08","title":"Version 0.3.1"},{"location":"changelog/#enhancements","text":"Move information on README to a site, organize and add more examples. Documentation is available at turicas.info/rows . Thanks to @ellisonleao for Sphinx implementation and @ramiroluz for new examples. Little code refactorings.","title":"Enhancements"},{"location":"changelog/#bug-fixes_3","text":"#200 Escape output when exporting to HTML (thanks to @arloc ) Fix some tests #215 DecimalField does not handle negative values correctly if using locale (thanks to @draug3n for reporting)","title":"Bug Fixes"},{"location":"changelog/#version-030","text":"Released on: 2016-09-02","title":"Version 0.3.0"},{"location":"changelog/#backwards-incompatible-changes","text":"","title":"Backwards Incompatible Changes"},{"location":"changelog/#bug-fixes_4","text":"Return None on XLS blank cells; #188 Change sample_size on encoding detection.","title":"Bug Fixes"},{"location":"changelog/#enhancements-and-refactorings","text":"rows.fields.detect_fields will consider BinaryField if all the values are str (Python 2)/ bytes (Python 3) and all other fields will work only with unicode (Python 2)/ str (Python 3); Plugins HTML and XPath now uses a better way to return inner HTML (when preserve_html=True ); #189 Optimize Table.__add__ .","title":"Enhancements and Refactorings"},{"location":"changelog/#new-features","text":"Support for Python 3 (finally!); rows.fields.BinaryField now automatically uses base64 to encode/decode; Added encoding information to rows.Table metadata in text plugins; Added sheet_name information to rows.Table metadata in XLS and XLSX plugins; #190 Add query_args to import_from_sqlite ; #177 Add dialect to export_to_csv .","title":"New Features"},{"location":"changelog/#version-021","text":"Released on: 2016-08-10","title":"Version 0.2.1"},{"location":"changelog/#backwards-incompatible-changes_1","text":"rows.utils.export_to_uri signature is now like rows.export_to_* (first the rows.Table object, then the URI) Changed default table name in import_from_sqlite and export_to_sqlite (from rows and rows_{number} to table{number} )","title":"Backwards Incompatible Changes"},{"location":"changelog/#bug-fixes_5","text":"#170 (SQLite plugin) Error converting int and float when value is None . #168 Use Field.serialize if does not know the field type (affecting: XLS, XLSX and SQLite plugins). #167 Use more data to detect dialect, delimit the possible delimiters and fallback to excel if can't detect. #176 Problem using quotes on CSV plugin. #179 Fix double underscore problem on rows.utils.slug #175 Fix None serialization/deserialization in all plugins (and also field types) #172 Expose all tables in rows query for SQLite databases Fix examples/cli/convert.sh (missing - ) Avoids SQL injection in table name","title":"Bug Fixes"},{"location":"changelog/#enhancements-and-refactorings_1","text":"Refactor rows.utils.import_from_uri Encoding and file type are better detected on rows.utils.import_from_uri Added helper functions to rows.utils regarding encoding and file type/plugin detection There's a better description of plugin metadata (MIME types accepted) on rows.utils (should be refactored to be inside each plugin) Moved slug and ipartition functions to rows.plugins.utils Optimize rows query when using only one SQLite source","title":"Enhancements and Refactorings"},{"location":"changelog/#version-020","text":"Released on: 2016-07-15","title":"Version 0.2.0"},{"location":"changelog/#backwards-incompatible-changes_2","text":"rows.fields.UnicodeField was renamed to rows.fields.TextField rows.fields.BytesField was renamed to rows.fields.BinaryField","title":"Backwards Incompatible Changes"},{"location":"changelog/#bug-fixes_6","text":"Fix import errors on older versions of urllib3 and Python (thanks to @jeanferri ) #156 BoolField should not accept \"0\" and \"1\" as possible values #86 Fix Content-Type parsing Fix locale-related tests #85 Fix preserve_html if fields is not provided Fix problem with big integers #131 Fix problem when empty sample data Fix problem with unicode and DateField Fix PercentField.serialize(None) Fix bug with Decimal receiving '' Fix bug in PercentField.serialize(Decimal('0')) Fix nested table behaviour on HTML plugin","title":"Bug Fixes"},{"location":"changelog/#general-changes","text":"(EXPERIMENTAL) Add rows.FlexibleTable class (with help on tests from @maurobaraildi ) Lots of refactorings Add rows.operations.transpose Add Table.__repr__ Renamte rows.fields.UnicodeField to rows.fields.TextField and rows.fields.ByteField to rows.fields.BinaryField Add a man page (thanks to @kretcheu ) #40 The package is available on Debian! #120 The package is available on Fedora! Add some examples #138 Add rows.fields.JSONField #146 Add rows.fields.EmailField Enhance encoding detection using file-magic library #160 Add support for column get/set/del in rows.Table","title":"General Changes"},{"location":"changelog/#tests","text":"Fix \"\\r\\n\" on tests to work on Windows Enhance tests with mock to assure some functions are being called Improve some tests","title":"Tests"},{"location":"changelog/#plugins_2","text":"Add plugin JSON (thanks @sxslex ) #107 Add import_from_txt #149 Add import_from_xpath (EXPERIMENTAL) Add import_from_ods (EXPERIMENTAL) Add import_from_parquet Add import_from_sqlite and export_to_sqlite (implemented by @turicas with help from @infog ) Add import_from_xlsx and export_to_xlsx (thanks to @RhenanBartels ) Autodetect delimiter in CSV files Export to TXT, JSON and XLS also support an already opened file and CSV can export to memory (thanks to @jeanferri ) #93 Add HTML helpers inside rows.plugins.html : count_tables , extract_text , extract_links and tag_to_dict #162 Add import_from_dicts and export_to_dicts Refactor export_to_txt","title":"Plugins"},{"location":"changelog/#utils_2","text":"Create rows.plugins.utils #119 Rename field name if name is duplicated (to \"field_2\", \"field_3\", ..., \"field_N\") or if starts with a number. Add option to import only some fields ( import_fields parameter inside create_table ) Add option to export only some fields ( export_fields parameter inside prepare_to_export ) Add option force_types to force field types in some columns (instead of detecting) on create_table . Support lazy objects on create_table Add samples parameter to create_table","title":"Utils"},{"location":"changelog/#cli","text":"Add option to disable SSL verification ( --verify-ssl=no ) Add print command Add --version CLI is not installed by default (should be installed as pip install rows[cli] ) Automatically detect default encoding (if not specified) Add --order-by to some commands and remove sort command. #111 Do not use locale by default Add query command: converts (from many sources) internally to SQLite, execute the query and then export","title":"CLI"},{"location":"changelog/#version-011","text":"Released on: 2015-09-03 Fix code to run on Windows (thanks @sxslex ) Fix locale (name, default name etc.) Remove filemagic dependency (waiting for python-magic to be available on PyPI) Write log of changes for 0.1.0 and 0.1.1","title":"Version 0.1.1"},{"location":"changelog/#version-010","text":"Released on: 2015-08-29 Implement Table and its basic methods Implement basic plugin support with many utilities and the following formats: csv (input/output) html (input/output) txt (output) xls (input/output) Implement the following field types - many of them with locale support: ByteField BoolField IntegerField FloatField DecimalField PercentField DateField DatetimeField UnicodeField Implement basic Table operations: sum join transform serialize Implement a command-line interface with the following commands: convert join sort sum Add examples to the repository","title":"Version 0.1.0"},{"location":"cli/","text":"Command-Line Interface rows exposes a command-line interface with common operations such as converting and querying data. Note: we still need to improve this documentation. Please run rows --help to see all the available commands, see the code reference or take a look at rows/cli.py . Man pages are also available . Getting started Before using the command-line interface, you must first install the dependencies required by the module: pip install rows[cli] Commands All the commands accepts any of the formats supported by the library (unless in some specific/optimized cases, like csv2sqlite , sqlite2csv , pgimport and pgexport ) and for all input data you can specify an URL instead of a local filename (example: rows convert https://website/file.html file.csv ). Note: you must install the specific dependencies for each format you want support (example: to extract tables from HTML the Python library lxml is required). rows convert : convert a table from one format to another. rows csv2sqlite : convert one or more CSV files (compressed or not) to SQLite in an optimized way (if source is CSV and destination is SQLite, use this rather than rows convert ). rows csv-merge : lazily merge CSV files (compressed or not), even if they don't have a common schema, generating a new one (compressed or not). rows join : equivalent to SQL's JOIN - get rows from each table and join them. rows pdf-to-text : extract text from a PDF file and save into a file or print to standard output; rows pgexport : export a PostgreSQL table into a CSV file (compressed or not) in the most optimized way: using psql 's COPY command. rows pgimport : import a CSV file (compressed or not) into a PostgreSQL table in the most optimized way: using psql 's COPY command. rows print : print a table to the standard output (you can choose between some frame styles). rows query : query a table using SQL (converts the table to an in-memory SQLite database) and output to the standard output or a file. rows schema : inspects a table and defines its schema. Can output in many formats, like text, SQL or even Django models. rows sqlite2csv : convert a SQLite table into a CSV file (compressed or not). rows sum : aggreate the rows of two equivalent tables (must have same field names and types), equivalent to SQL's UNION . Note: everytime we specify \"compressed or not\" means you can use the file as is or a compressed version of it. The supported compression formats are: gzip ( .gz ), lzma ( .xz ) and bzip2 ( .bz2 ). Support for archive formats such as zip, tar and rar will be implemented in the future . Global and Common Parameters Some parameters are global to the command-line interface and the sub-commands also have specific options. The global options are: --http-cache=BOOLEAN : Enable/disable HTTP cache (default: true ) --http-cache-path=TEXT : Set HTTP cache path (default: USER_HOME_PATH/.cache/rows/http rows convert Convert a table from a source URI to destination . Useful to convert files between formats, like extracting data from a HTML table and converting to CSV. Note: if you'd like to convert from/to CSV, SQLite or PostgreSQL, see the more optimized commands csv2sqlite , sqlite2csv , pgimport and pgexport . Usage: rows convert [OPTIONS] SOURCE DESTINATION Options: --input-encoding=TEXT : Encoding of input tables (default: utf-8 ) --output-encoding=TEXT : Encoding of output tables (default: utf-8 ) --input-locale=TEXT : Locale of input tables. Used to parse integers, floats etc. (default: C ) --output-locale=TEXT : Locale of output tables. Used to parse integers, floats etc. (default: C ) --verify-ssl=BOOLEAN : Verify SSL certificate, if source is downloaded via HTTPS (default: true ) --order-by=TEXT : Order result by this field (default: same order as input data) --fields=TEXT : A comma-separated list of fields to import (default: all fields) --fields-exclude=TEXT : A comma-separated list of fields to exclude when exporting (default: none) Examples: # needs: pip install rows[html] rows convert \\ http://www.sports-reference.com/olympics/countries/BRA/summer/2016/ \\ brazil-2016.csv rows convert \\ http://www.worldometers.info/world-population/population-by-country/ \\ population.csv rows csv-merge Lazily merge CSV files (compressed or not), even if they don't have a common schema, generating a new one (compressed or not). The final header is slugified. Sources and destination could be compressed, the supported compression formats are: gzip ( .gz ), lzma ( .xz ) and bzip2 ( .bz2 ). This command is splited in two main steps: Identify dialect and headers for each source file, defining the final header (it's a union of all field names slugged); Lazily read all sources and write a new CSV using the defined header. Usage: rows csv-merge [OPTIONS] SOURCES... OUTPUT Options: --input-encoding=TEXT : input encoding for all CSV files (default: utf-8 ) --output-encoding=TEXT : encoding of output CSV (default: utf-8 ) --strip : remove spaces from CSV cells Example: rows csv-merge \\ file1.csv file2.csv.bz2 file3.csv.xz \\ result.csv.gz rows csv2sqlite Convert one or more CSV files (compressed or not) to SQLite in an optimized way (if source is CSV and destination is SQLite, use this rather than rows convert ). The supported compression formats are: gzip ( .gz ), lzma ( .xz ) and bzip2 ( .bz2 ). Usage: rows csv2sqlite [OPTIONS] SOURCES... OUTPUT Options: --batch-size=INTEGER : number of rows to batch insert into SQLite (default: 10000 ) --samples=INTEGER : number of sample rows to detect schema (default: 5000 ) --input-encoding=TEXT : input encoding (default: utf-8 ) --dialect=TEXT : CSV dialect to be used (default: will detect automatically) --schemas=TEXT : comma-separated list of schema files (default: will detect automatically) - these files must have the columns field_name and field_type (you can see and example by running rows schema ) Example: rows csv2sqlite \\ --dialect=excel \\ --input-encoding=latin1 \\ file1.csv file2.csv \\ result.sqlite rows join Join tables from source URIs using key(s) to group rows and save into destination . This command is not optimized and its use is discouraged ( rows query may be more effective). Usage: rows join [OPTIONS] KEYS SOURCES... DESTINATION Options: --input-encoding=TEXT : Encoding of input tables (default: utf-8 ) --output-encoding=TEXT : Encoding of output tables (default: utf-8 ) --input-locale=TEXT : Locale of input tables. Used to parse integers, floats etc. (default: C ) --output-locale=TEXT : Locale of output tables. Used to parse integers, floats etc. (default: C ) --verify-ssl=BOOLEAN : Verify SSL certificate, if source is downloaded via HTTPS (default: true ) --order-by=TEXT : Order result by this field (default: same order as input data) --fields=TEXT : A comma-separated list of fields to import (default: all fields) --fields-exclude=TEXT : A comma-separated list of fields to exclude when exporting (default: none) Example: join a.csv and b.csv into a new file called c.csv using the field id as a key (both a.csv and b.csv must have the field id ): rows join id a.csv b.csv c.csv rows pdf-to-text Extract text from a PDF file and save into a file or print to standard output. Usage: rows pdf-to-text [OPTIONS] SOURCE [OUTPUT] Options: --output-encoding=TEXT : encoding to be used on output file (default: utf-8 ) - valid only when output is specified (if not, uses the default standard output's encoding) --quiet : do not show progress bars when downloading and extracting. This option is automatically disabled if output is empty (default: show progress bars) --backend=TEXT : PDF library to use as backend (default: pymupdf ) --pages=TEXT : page ranges Example: # needs: pip install rows[pdf] URL=\"http://www.imprensaoficial.rr.gov.br/app/_edicoes/2018/01/doe-20180131.pdf\" rows pdf-to-text $URL result.txt # Save to file, show progress bars rows pdf-to-text --quiet $URL result.txt # Save to file, no progress bars rows pdf-to-text --pages=1,2,3 $URL # Print first 3 pages to stdout rows pdf-to-text --pages=1-3 $URL # Print first 3 pages to stdout (using ranges) rows pgexport Export a PostgreSQL table into a CSV file (compressed or not) in the most optimized way: using psql 's COPY command. The supported compression formats are: gzip ( .gz ), lzma ( .xz ) and bzip2 ( .bz2 ). Usage: rows pgexport [OPTIONS] DATABASE_URI TABLE_NAME DESTINATION Options: --output-encoding=TEXT : encoding to be used on output file (default: utf-8 ) --dialect=TEXT : CSV dialect to be used on output file (default: excel ) Example: # needs: pip install rows[postgresql] rows pgexport \\ postgres://postgres:postgres@127.0.0.1:42001/rows \\ my_table \\ my_table.csv.gz rows pgimport Import a CSV file (compressed or not) into a PostgreSQL table in the most optimized way: using psql 's COPY command. The supported compression formats are: gzip ( .gz ), lzma ( .xz ) and bzip2 ( .bz2 ). Usage: rows pgimport [OPTIONS] SOURCE DATABASE_URI TABLE_NAME Options: --input-encoding=TEXT : Encoding of input CSV file (default: utf-8 ) --no-create-table=BOOLEAN : should rows create the table or leave it to PostgreSQL? (default: false, ie: create the table) --dialect=TEXT : CSV dialect to be used (default: will detect automatically) --schema=TEXT : schema filename to be used (default: will detect schema automatically) - this file must have the columns field_name and field_type (you can see and example by running rows schema ) --unlogged : if specified, create an unlogged table (which is faster than logged ones, but will not be recoverable in case of data corruption and will not be sent to replicas) Example: # needs: pip install rows[postgresql] rows pgimport \\ my_data.csv.xz \\ postgres://postgres:postgres@127.0.0.1:42001/rows \\ my_table Tip : whenever possible, specify --schema , --dialect and --input-encoding so the library won't try to guess it (the guessing process could take a while). Note : if you're importing a gzip-compressed CSV which has the uncompressed size greater than 4GB, then the progress bar total won't have the correct value at first; once the progress bar reaches 100%, it'll check if this value is the correct one and then either finish the process (if correct) or update with a new possible total. rows cannot do any better than this, since there's a limitation on the gzip format: it stores the uncompressed size modulo 2^32, so if the size is bigger than 4GB (2^32), the best thing rows can do is to try to guess the leftmost bits truncated from the uncompressed size number. rows print Print the selected source table Usage: rows print [OPTIONS] SOURCE Options: --input-encoding=TEXT : Encoding of input tables (default: utf-8 ) --output-encoding=TEXT : Encoding of output tables (default: utf-8 ) --input-locale=TEXT : Locale of input tables. Used to parse integers, floats etc. (default: C ) --output-locale=TEXT : Locale of output tables. Used to parse integers, floats etc. (default: C ) --verify-ssl=BOOLEAN : Verify SSL certificate, if source is downloaded via HTTPS (default: true ) --order-by=TEXT : Order result by this field (default: same order as input data) --fields=TEXT : A comma-separated list of fields to import (default: all fields) --fields-exclude=TEXT : A comma-separated list of fields to exclude when exporting (default: none) --frame-style=TEXT : frame style to \"draw\" the table; options: ascii , single , double , none (default: ascii ) --table-index=INTEGER : if source is HTML, specify the table index to extract (default: 0 , ie: first <table> inside the HTML file) Examples: rows print \\ --fields=state,city \\ --order-by=city \\ data/brazilian-cities.csv Note: download brazilian-cities.csv . # needs: pip install rows[html] rows print \\ --table-index=1 \\ # extracts second table some-html-file.html rows query Yep, you can SQL-query any supported file format! Each of the source files will be a table inside an in-memory SQLite database, called table1 , ..., tableN . If the --output is not specified, rows will print a table to the standard output. Usage: rows query [OPTIONS] QUERY SOURCES... Options: --input-encoding=TEXT : Encoding of input tables (default: utf-8 ) --output-encoding=TEXT : Encoding of output tables (default: utf-8 ) --input-locale=TEXT : Locale of input tables. Used to parse integers, floats etc. (default: C ) --output-locale=TEXT : Locale of output tables. Used to parse integers, floats etc. (default: C ) --verify-ssl=BOOLEAN : Verify SSL certificate, if source is downloaded via HTTPS (default: true ) --samples=INTEGER : number of sample rows to detect schema (default: 5000 ) --output=TEXT : filename to outputs - will use file extension to define which plugin to use (default: standard output, plugin text) --frame-style=TEXT : frame style to \"draw\" the table; options: ascii , single , double , none (default: ascii ) Examples: # needs: pip install rows[html] rows query \\ \"SELECT * FROM table1 WHERE inhabitants > 1000000\" \\ data/brazilian-cities.csv \\ --output=data/result.html Note: download brazilian-cities.csv . # needs: pip install rows[pdf] rows query \\ 'SELECT * FROM table1 WHERE categoria = \"Impr\u00f3pria\"' \\ http://balneabilidade.inema.ba.gov.br/index.php/relatoriodebalneabilidade/geraBoletim?idcampanha=36381 \\ --output=bathing-conditions.xls In the last example rows will: Download a file using HTTP Identify its format (PDF) Automatically extract a table based on objects' positions Create an in-memory database with extracted data Run the SQL query Export the result to XLS In just one command, automatically. How crazy is that? rows schema Identifies the table schema by inspecting data. The files generated by this command ( txt format) can be used in --schema and --schemas options (a CSV version of these files can also be used). Usage: rows schema [OPTIONS] SOURCE [OUTPUT] Options: --input-encoding=TEXT : Encoding of input tables (default: utf-8 ) --input-locale=TEXT : Locale of input tables. Used to parse integers, floats etc. (default: C ) --verify-ssl=BOOLEAN : Verify SSL certificate, if source is downloaded via HTTPS (default: true ) -f TEXT , --format=TEXT : output format; options: txt , sql , django (default: txt ) --fields=TEXT : A comma-separated list of fields to import (default: all fields) --fields-exclude=TEXT : A comma-separated list of fields to exclude when exporting (default: none) --samples=INTEGER : number of sample rows to detect schema (default: 5000 ) Example: rows schema --samples=100 data/brazilian-cities.csv Note: download brazilian-cities.csv . Output: +-------------+------------+ | field_name | field_type | +-------------+------------+ | state | text | | city | text | | inhabitants | integer | | area | float | +-------------+------------+ rows sqlite2csv Convert a SQLite table into a CSV file (compressed or not). The supported compression formats are: gzip ( .gz ), lzma ( .xz ) and bzip2 ( .bz2 ). Usage: rows sqlite2csv [OPTIONS] SOURCE TABLE_NAME OUTPUT Options: --batch-size=INTEGER : number of rows to batch insert into SQLite (default: 10000 ) --dialect=TEXT : CSV dialect to be used on output file (default: excel ) Example: rows sqlite2csv my_db.sqlite my_table my_table.csv.bz2 rows sum Sum tables (append rows from one to the other) from source URIs and save into destination . The tables must have the same fields. This command is not optimized and its use is discouraged ( rows query may be more effective). Usage: rows sum [OPTIONS] SOURCES... DESTINATION Options: --input-encoding=TEXT : Encoding of input tables (default: utf-8 ) --output-encoding=TEXT : Encoding of output tables (default: utf-8 ) --input-locale=TEXT : Locale of input tables. Used to parse integers, floats etc. (default: C ) --output-locale=TEXT : Locale of output tables. Used to parse integers, floats etc. (default: C ) --verify-ssl=BOOLEAN : Verify SSL certificate, if source is downloaded via HTTPS (default: true ) --order-by=TEXT : Order result by this field (default: same order as input data) --fields=TEXT : A comma-separated list of fields to import (default: all fields) --fields-exclude=TEXT : A comma-separated list of fields to exclude when exporting (default: none) Example: rows sum \\ --fields=id,name,phone \\ people.csv \\ # This file has `id`, `name` and other fields phones.csv \\ # This file has `id`, `phone` and other fields contacts.csv # Will have `id`, `name` and `phone` fields","title":"Command-Line"},{"location":"cli/#command-line-interface","text":"rows exposes a command-line interface with common operations such as converting and querying data. Note: we still need to improve this documentation. Please run rows --help to see all the available commands, see the code reference or take a look at rows/cli.py . Man pages are also available .","title":"Command-Line Interface"},{"location":"cli/#getting-started","text":"Before using the command-line interface, you must first install the dependencies required by the module: pip install rows[cli]","title":"Getting started"},{"location":"cli/#commands","text":"All the commands accepts any of the formats supported by the library (unless in some specific/optimized cases, like csv2sqlite , sqlite2csv , pgimport and pgexport ) and for all input data you can specify an URL instead of a local filename (example: rows convert https://website/file.html file.csv ). Note: you must install the specific dependencies for each format you want support (example: to extract tables from HTML the Python library lxml is required). rows convert : convert a table from one format to another. rows csv2sqlite : convert one or more CSV files (compressed or not) to SQLite in an optimized way (if source is CSV and destination is SQLite, use this rather than rows convert ). rows csv-merge : lazily merge CSV files (compressed or not), even if they don't have a common schema, generating a new one (compressed or not). rows join : equivalent to SQL's JOIN - get rows from each table and join them. rows pdf-to-text : extract text from a PDF file and save into a file or print to standard output; rows pgexport : export a PostgreSQL table into a CSV file (compressed or not) in the most optimized way: using psql 's COPY command. rows pgimport : import a CSV file (compressed or not) into a PostgreSQL table in the most optimized way: using psql 's COPY command. rows print : print a table to the standard output (you can choose between some frame styles). rows query : query a table using SQL (converts the table to an in-memory SQLite database) and output to the standard output or a file. rows schema : inspects a table and defines its schema. Can output in many formats, like text, SQL or even Django models. rows sqlite2csv : convert a SQLite table into a CSV file (compressed or not). rows sum : aggreate the rows of two equivalent tables (must have same field names and types), equivalent to SQL's UNION . Note: everytime we specify \"compressed or not\" means you can use the file as is or a compressed version of it. The supported compression formats are: gzip ( .gz ), lzma ( .xz ) and bzip2 ( .bz2 ). Support for archive formats such as zip, tar and rar will be implemented in the future .","title":"Commands"},{"location":"cli/#global-and-common-parameters","text":"Some parameters are global to the command-line interface and the sub-commands also have specific options. The global options are: --http-cache=BOOLEAN : Enable/disable HTTP cache (default: true ) --http-cache-path=TEXT : Set HTTP cache path (default: USER_HOME_PATH/.cache/rows/http","title":"Global and Common Parameters"},{"location":"cli/#rows-convert","text":"Convert a table from a source URI to destination . Useful to convert files between formats, like extracting data from a HTML table and converting to CSV. Note: if you'd like to convert from/to CSV, SQLite or PostgreSQL, see the more optimized commands csv2sqlite , sqlite2csv , pgimport and pgexport . Usage: rows convert [OPTIONS] SOURCE DESTINATION Options: --input-encoding=TEXT : Encoding of input tables (default: utf-8 ) --output-encoding=TEXT : Encoding of output tables (default: utf-8 ) --input-locale=TEXT : Locale of input tables. Used to parse integers, floats etc. (default: C ) --output-locale=TEXT : Locale of output tables. Used to parse integers, floats etc. (default: C ) --verify-ssl=BOOLEAN : Verify SSL certificate, if source is downloaded via HTTPS (default: true ) --order-by=TEXT : Order result by this field (default: same order as input data) --fields=TEXT : A comma-separated list of fields to import (default: all fields) --fields-exclude=TEXT : A comma-separated list of fields to exclude when exporting (default: none) Examples: # needs: pip install rows[html] rows convert \\ http://www.sports-reference.com/olympics/countries/BRA/summer/2016/ \\ brazil-2016.csv rows convert \\ http://www.worldometers.info/world-population/population-by-country/ \\ population.csv","title":"rows convert"},{"location":"cli/#rows-csv-merge","text":"Lazily merge CSV files (compressed or not), even if they don't have a common schema, generating a new one (compressed or not). The final header is slugified. Sources and destination could be compressed, the supported compression formats are: gzip ( .gz ), lzma ( .xz ) and bzip2 ( .bz2 ). This command is splited in two main steps: Identify dialect and headers for each source file, defining the final header (it's a union of all field names slugged); Lazily read all sources and write a new CSV using the defined header. Usage: rows csv-merge [OPTIONS] SOURCES... OUTPUT Options: --input-encoding=TEXT : input encoding for all CSV files (default: utf-8 ) --output-encoding=TEXT : encoding of output CSV (default: utf-8 ) --strip : remove spaces from CSV cells Example: rows csv-merge \\ file1.csv file2.csv.bz2 file3.csv.xz \\ result.csv.gz","title":"rows csv-merge"},{"location":"cli/#rows-csv2sqlite","text":"Convert one or more CSV files (compressed or not) to SQLite in an optimized way (if source is CSV and destination is SQLite, use this rather than rows convert ). The supported compression formats are: gzip ( .gz ), lzma ( .xz ) and bzip2 ( .bz2 ). Usage: rows csv2sqlite [OPTIONS] SOURCES... OUTPUT Options: --batch-size=INTEGER : number of rows to batch insert into SQLite (default: 10000 ) --samples=INTEGER : number of sample rows to detect schema (default: 5000 ) --input-encoding=TEXT : input encoding (default: utf-8 ) --dialect=TEXT : CSV dialect to be used (default: will detect automatically) --schemas=TEXT : comma-separated list of schema files (default: will detect automatically) - these files must have the columns field_name and field_type (you can see and example by running rows schema ) Example: rows csv2sqlite \\ --dialect=excel \\ --input-encoding=latin1 \\ file1.csv file2.csv \\ result.sqlite","title":"rows csv2sqlite"},{"location":"cli/#rows-join","text":"Join tables from source URIs using key(s) to group rows and save into destination . This command is not optimized and its use is discouraged ( rows query may be more effective). Usage: rows join [OPTIONS] KEYS SOURCES... DESTINATION Options: --input-encoding=TEXT : Encoding of input tables (default: utf-8 ) --output-encoding=TEXT : Encoding of output tables (default: utf-8 ) --input-locale=TEXT : Locale of input tables. Used to parse integers, floats etc. (default: C ) --output-locale=TEXT : Locale of output tables. Used to parse integers, floats etc. (default: C ) --verify-ssl=BOOLEAN : Verify SSL certificate, if source is downloaded via HTTPS (default: true ) --order-by=TEXT : Order result by this field (default: same order as input data) --fields=TEXT : A comma-separated list of fields to import (default: all fields) --fields-exclude=TEXT : A comma-separated list of fields to exclude when exporting (default: none) Example: join a.csv and b.csv into a new file called c.csv using the field id as a key (both a.csv and b.csv must have the field id ): rows join id a.csv b.csv c.csv","title":"rows join"},{"location":"cli/#rows-pdf-to-text","text":"Extract text from a PDF file and save into a file or print to standard output. Usage: rows pdf-to-text [OPTIONS] SOURCE [OUTPUT] Options: --output-encoding=TEXT : encoding to be used on output file (default: utf-8 ) - valid only when output is specified (if not, uses the default standard output's encoding) --quiet : do not show progress bars when downloading and extracting. This option is automatically disabled if output is empty (default: show progress bars) --backend=TEXT : PDF library to use as backend (default: pymupdf ) --pages=TEXT : page ranges Example: # needs: pip install rows[pdf] URL=\"http://www.imprensaoficial.rr.gov.br/app/_edicoes/2018/01/doe-20180131.pdf\" rows pdf-to-text $URL result.txt # Save to file, show progress bars rows pdf-to-text --quiet $URL result.txt # Save to file, no progress bars rows pdf-to-text --pages=1,2,3 $URL # Print first 3 pages to stdout rows pdf-to-text --pages=1-3 $URL # Print first 3 pages to stdout (using ranges)","title":"rows pdf-to-text"},{"location":"cli/#rows-pgexport","text":"Export a PostgreSQL table into a CSV file (compressed or not) in the most optimized way: using psql 's COPY command. The supported compression formats are: gzip ( .gz ), lzma ( .xz ) and bzip2 ( .bz2 ). Usage: rows pgexport [OPTIONS] DATABASE_URI TABLE_NAME DESTINATION Options: --output-encoding=TEXT : encoding to be used on output file (default: utf-8 ) --dialect=TEXT : CSV dialect to be used on output file (default: excel ) Example: # needs: pip install rows[postgresql] rows pgexport \\ postgres://postgres:postgres@127.0.0.1:42001/rows \\ my_table \\ my_table.csv.gz","title":"rows pgexport"},{"location":"cli/#rows-pgimport","text":"Import a CSV file (compressed or not) into a PostgreSQL table in the most optimized way: using psql 's COPY command. The supported compression formats are: gzip ( .gz ), lzma ( .xz ) and bzip2 ( .bz2 ). Usage: rows pgimport [OPTIONS] SOURCE DATABASE_URI TABLE_NAME Options: --input-encoding=TEXT : Encoding of input CSV file (default: utf-8 ) --no-create-table=BOOLEAN : should rows create the table or leave it to PostgreSQL? (default: false, ie: create the table) --dialect=TEXT : CSV dialect to be used (default: will detect automatically) --schema=TEXT : schema filename to be used (default: will detect schema automatically) - this file must have the columns field_name and field_type (you can see and example by running rows schema ) --unlogged : if specified, create an unlogged table (which is faster than logged ones, but will not be recoverable in case of data corruption and will not be sent to replicas) Example: # needs: pip install rows[postgresql] rows pgimport \\ my_data.csv.xz \\ postgres://postgres:postgres@127.0.0.1:42001/rows \\ my_table Tip : whenever possible, specify --schema , --dialect and --input-encoding so the library won't try to guess it (the guessing process could take a while). Note : if you're importing a gzip-compressed CSV which has the uncompressed size greater than 4GB, then the progress bar total won't have the correct value at first; once the progress bar reaches 100%, it'll check if this value is the correct one and then either finish the process (if correct) or update with a new possible total. rows cannot do any better than this, since there's a limitation on the gzip format: it stores the uncompressed size modulo 2^32, so if the size is bigger than 4GB (2^32), the best thing rows can do is to try to guess the leftmost bits truncated from the uncompressed size number.","title":"rows pgimport"},{"location":"cli/#rows-print","text":"Print the selected source table Usage: rows print [OPTIONS] SOURCE Options: --input-encoding=TEXT : Encoding of input tables (default: utf-8 ) --output-encoding=TEXT : Encoding of output tables (default: utf-8 ) --input-locale=TEXT : Locale of input tables. Used to parse integers, floats etc. (default: C ) --output-locale=TEXT : Locale of output tables. Used to parse integers, floats etc. (default: C ) --verify-ssl=BOOLEAN : Verify SSL certificate, if source is downloaded via HTTPS (default: true ) --order-by=TEXT : Order result by this field (default: same order as input data) --fields=TEXT : A comma-separated list of fields to import (default: all fields) --fields-exclude=TEXT : A comma-separated list of fields to exclude when exporting (default: none) --frame-style=TEXT : frame style to \"draw\" the table; options: ascii , single , double , none (default: ascii ) --table-index=INTEGER : if source is HTML, specify the table index to extract (default: 0 , ie: first <table> inside the HTML file) Examples: rows print \\ --fields=state,city \\ --order-by=city \\ data/brazilian-cities.csv Note: download brazilian-cities.csv . # needs: pip install rows[html] rows print \\ --table-index=1 \\ # extracts second table some-html-file.html","title":"rows print"},{"location":"cli/#rows-query","text":"Yep, you can SQL-query any supported file format! Each of the source files will be a table inside an in-memory SQLite database, called table1 , ..., tableN . If the --output is not specified, rows will print a table to the standard output. Usage: rows query [OPTIONS] QUERY SOURCES... Options: --input-encoding=TEXT : Encoding of input tables (default: utf-8 ) --output-encoding=TEXT : Encoding of output tables (default: utf-8 ) --input-locale=TEXT : Locale of input tables. Used to parse integers, floats etc. (default: C ) --output-locale=TEXT : Locale of output tables. Used to parse integers, floats etc. (default: C ) --verify-ssl=BOOLEAN : Verify SSL certificate, if source is downloaded via HTTPS (default: true ) --samples=INTEGER : number of sample rows to detect schema (default: 5000 ) --output=TEXT : filename to outputs - will use file extension to define which plugin to use (default: standard output, plugin text) --frame-style=TEXT : frame style to \"draw\" the table; options: ascii , single , double , none (default: ascii ) Examples: # needs: pip install rows[html] rows query \\ \"SELECT * FROM table1 WHERE inhabitants > 1000000\" \\ data/brazilian-cities.csv \\ --output=data/result.html Note: download brazilian-cities.csv . # needs: pip install rows[pdf] rows query \\ 'SELECT * FROM table1 WHERE categoria = \"Impr\u00f3pria\"' \\ http://balneabilidade.inema.ba.gov.br/index.php/relatoriodebalneabilidade/geraBoletim?idcampanha=36381 \\ --output=bathing-conditions.xls In the last example rows will: Download a file using HTTP Identify its format (PDF) Automatically extract a table based on objects' positions Create an in-memory database with extracted data Run the SQL query Export the result to XLS In just one command, automatically. How crazy is that?","title":"rows query"},{"location":"cli/#rows-schema","text":"Identifies the table schema by inspecting data. The files generated by this command ( txt format) can be used in --schema and --schemas options (a CSV version of these files can also be used). Usage: rows schema [OPTIONS] SOURCE [OUTPUT] Options: --input-encoding=TEXT : Encoding of input tables (default: utf-8 ) --input-locale=TEXT : Locale of input tables. Used to parse integers, floats etc. (default: C ) --verify-ssl=BOOLEAN : Verify SSL certificate, if source is downloaded via HTTPS (default: true ) -f TEXT , --format=TEXT : output format; options: txt , sql , django (default: txt ) --fields=TEXT : A comma-separated list of fields to import (default: all fields) --fields-exclude=TEXT : A comma-separated list of fields to exclude when exporting (default: none) --samples=INTEGER : number of sample rows to detect schema (default: 5000 ) Example: rows schema --samples=100 data/brazilian-cities.csv Note: download brazilian-cities.csv . Output: +-------------+------------+ | field_name | field_type | +-------------+------------+ | state | text | | city | text | | inhabitants | integer | | area | float | +-------------+------------+","title":"rows schema"},{"location":"cli/#rows-sqlite2csv","text":"Convert a SQLite table into a CSV file (compressed or not). The supported compression formats are: gzip ( .gz ), lzma ( .xz ) and bzip2 ( .bz2 ). Usage: rows sqlite2csv [OPTIONS] SOURCE TABLE_NAME OUTPUT Options: --batch-size=INTEGER : number of rows to batch insert into SQLite (default: 10000 ) --dialect=TEXT : CSV dialect to be used on output file (default: excel ) Example: rows sqlite2csv my_db.sqlite my_table my_table.csv.bz2","title":"rows sqlite2csv"},{"location":"cli/#rows-sum","text":"Sum tables (append rows from one to the other) from source URIs and save into destination . The tables must have the same fields. This command is not optimized and its use is discouraged ( rows query may be more effective). Usage: rows sum [OPTIONS] SOURCES... DESTINATION Options: --input-encoding=TEXT : Encoding of input tables (default: utf-8 ) --output-encoding=TEXT : Encoding of output tables (default: utf-8 ) --input-locale=TEXT : Locale of input tables. Used to parse integers, floats etc. (default: C ) --output-locale=TEXT : Locale of output tables. Used to parse integers, floats etc. (default: C ) --verify-ssl=BOOLEAN : Verify SSL certificate, if source is downloaded via HTTPS (default: true ) --order-by=TEXT : Order result by this field (default: same order as input data) --fields=TEXT : A comma-separated list of fields to import (default: all fields) --fields-exclude=TEXT : A comma-separated list of fields to exclude when exporting (default: none) Example: rows sum \\ --fields=id,name,phone \\ people.csv \\ # This file has `id`, `name` and other fields phones.csv \\ # This file has `id`, `phone` and other fields contacts.csv # Will have `id`, `name` and `phone` fields","title":"rows sum"},{"location":"contributing/","text":"Contributing Creating your development environment The preferred way is to create a virtualenv (you can do by using virtualenv, virtualenvwrapper, pyenv or whatever tool you'd like). Create the virtualenv: mkvirtualenv rows Install all plugins' dependencies: pip install --editable .[all] Install development dependencies: pip install -r requirements-development.txt Running the tests There are two possible ways of running the tests: on your own virtualenv or for each Python version. For the PostgreSQL plugin you're going to need a PostgreSQL server running and must set the POSTGRESQL_URI environment variable. If you have docker installed you can easily create a container running PostgreSQL with the provided docker-compose.yml by running: docker-compose -p rows -f docker-compose.yml up -d Running on your virtualenv nosetests -dsv --with-yanc --with-coverage --cover-package rows tests/*.py Running for all Python versions Run tests: make test or (if you don't have make ): tox you can also run tox against an specific python version: tox -e py27 tox -e py35 tox known issues : running tox with py27 environ may raise InvocationError in non Linux environments. To avoid it you may rebuild tox environment in every run with tox -e py27 -r or if you want to run nosetests directly (see last section). Running PostgreSQL tests A PostgreSQL server is needed to run the PostgreSQL plugin tests. You can use Docker to easily run a PostgreSQL server, but can also use your own method to run it. The POSTGRESQL_URI environment variable need to be se so you can run the tests. Running the PostgreSQL container using docker-compose, set the environment variable and run the PostgreSQL-specific tests: docker-compose -p rows -f docker-compose.yml up -d export POSTGRESQL_URI=postgres://postgres:postgres@127.0.0.1:42001/rows nosetests -dsv --with-yanc --with-coverage --cover-package rows tests/tests_plugin_postgresql.py Generating the documentation Just run: make docs And check the docs-build/ directory. You can also serve it via HTTP: make docs-serve Releasing new versions # X = next version number # *** Create the release branch git checkout -b release/X # Update docs/changelog.md & commit # Change version number in `setup.py` and `rows/__init__.py` & commit # *** Merge into master, tag and test it git checkout master && git merge --no-ff release/X git tag -a X make test # STOP HERE IF TESTS FAIL # *** Release to PyPI and docs to GitHub Pages make release make docs-upload # *** Update remote repository git branch -d release/X git push turicas master git checkout develop # Change version number in `setup.py` and `rows/__init__.py` to the next # dev version (eg: if released 0.4.1, then change to 0.4.2dev0) & commit git push turicas develop","title":"Contributing"},{"location":"contributing/#contributing","text":"","title":"Contributing"},{"location":"contributing/#creating-your-development-environment","text":"The preferred way is to create a virtualenv (you can do by using virtualenv, virtualenvwrapper, pyenv or whatever tool you'd like). Create the virtualenv: mkvirtualenv rows Install all plugins' dependencies: pip install --editable .[all] Install development dependencies: pip install -r requirements-development.txt","title":"Creating your development environment"},{"location":"contributing/#running-the-tests","text":"There are two possible ways of running the tests: on your own virtualenv or for each Python version. For the PostgreSQL plugin you're going to need a PostgreSQL server running and must set the POSTGRESQL_URI environment variable. If you have docker installed you can easily create a container running PostgreSQL with the provided docker-compose.yml by running: docker-compose -p rows -f docker-compose.yml up -d","title":"Running the tests"},{"location":"contributing/#running-on-your-virtualenv","text":"nosetests -dsv --with-yanc --with-coverage --cover-package rows tests/*.py","title":"Running on your virtualenv"},{"location":"contributing/#running-for-all-python-versions","text":"Run tests: make test or (if you don't have make ): tox you can also run tox against an specific python version: tox -e py27 tox -e py35 tox known issues : running tox with py27 environ may raise InvocationError in non Linux environments. To avoid it you may rebuild tox environment in every run with tox -e py27 -r or if you want to run nosetests directly (see last section).","title":"Running for all Python versions"},{"location":"contributing/#running-postgresql-tests","text":"A PostgreSQL server is needed to run the PostgreSQL plugin tests. You can use Docker to easily run a PostgreSQL server, but can also use your own method to run it. The POSTGRESQL_URI environment variable need to be se so you can run the tests. Running the PostgreSQL container using docker-compose, set the environment variable and run the PostgreSQL-specific tests: docker-compose -p rows -f docker-compose.yml up -d export POSTGRESQL_URI=postgres://postgres:postgres@127.0.0.1:42001/rows nosetests -dsv --with-yanc --with-coverage --cover-package rows tests/tests_plugin_postgresql.py","title":"Running PostgreSQL tests"},{"location":"contributing/#generating-the-documentation","text":"Just run: make docs And check the docs-build/ directory. You can also serve it via HTTP: make docs-serve","title":"Generating the documentation"},{"location":"contributing/#releasing-new-versions","text":"# X = next version number # *** Create the release branch git checkout -b release/X # Update docs/changelog.md & commit # Change version number in `setup.py` and `rows/__init__.py` & commit # *** Merge into master, tag and test it git checkout master && git merge --no-ff release/X git tag -a X make test # STOP HERE IF TESTS FAIL # *** Release to PyPI and docs to GitHub Pages make release make docs-upload # *** Update remote repository git branch -d release/X git push turicas master git checkout develop # Change version number in `setup.py` and `rows/__init__.py` to the next # dev version (eg: if released 0.4.1, then change to 0.4.2dev0) & commit git push turicas develop","title":"Releasing new versions"},{"location":"installation/","text":"Installation PyPI pip install rows GitHub pip install \"https://github.com/turicas/rows/archive/develop.zip#egg=rows\" # or (needs git) pip install \"git+https://github.com/turicas/rows.git@develop#egg=rows\" or: git clone https://github.com/turicas/rows.git cd rows python setup.py install The use of virtualenv is recommended. You can create a development image using Docker: cat Dockerfile | docker build -t turicas/rows:latest - Debian If you use Debian sid or testing you can install it directly from the main repository by running: apt install python-rows # Python library only apt install rows # Python library + CLI You may need to install SQLite too (on Ubuntu, for example). Fedora dnf install python-row # Python library + CLI Docker If you don't want to install on your machine but you'd like to try the library, there's a docker image available: mkdir -p data # Put your files here echo -e \"a,b\\n1,2\\n3,4\" > data/test.csv # To access the IPython shell: docker run --rm -it -v $(pwd)/data:/data turicas/rows:0.4.0 ipython # To access the command-line interface docker run --rm -it -v $(pwd)/data:/data turicas/rows:0.4.0 rows print /data/test.csv Installing plugins The plugins csv , dicts , json , sqlite and txt are built-in by default but if you want to use another one you need to explicitly install its dependencies, for example: pip install rows[html] pip install rows[xls] Note: if you're running another command line interpreter (like zsh) you may need to escape the characters [ and ] . You also need to install some dependencies to use the command-line interface . You can do it installing the cli extra requirement: pip install rows[cli] And - easily - you can install all the dependencies by using the all extra requirement: pip install rows[all]","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#pypi","text":"pip install rows","title":"PyPI"},{"location":"installation/#github","text":"pip install \"https://github.com/turicas/rows/archive/develop.zip#egg=rows\" # or (needs git) pip install \"git+https://github.com/turicas/rows.git@develop#egg=rows\" or: git clone https://github.com/turicas/rows.git cd rows python setup.py install The use of virtualenv is recommended. You can create a development image using Docker: cat Dockerfile | docker build -t turicas/rows:latest -","title":"GitHub"},{"location":"installation/#debian","text":"If you use Debian sid or testing you can install it directly from the main repository by running: apt install python-rows # Python library only apt install rows # Python library + CLI You may need to install SQLite too (on Ubuntu, for example).","title":"Debian"},{"location":"installation/#fedora","text":"dnf install python-row # Python library + CLI","title":"Fedora"},{"location":"installation/#docker","text":"If you don't want to install on your machine but you'd like to try the library, there's a docker image available: mkdir -p data # Put your files here echo -e \"a,b\\n1,2\\n3,4\" > data/test.csv # To access the IPython shell: docker run --rm -it -v $(pwd)/data:/data turicas/rows:0.4.0 ipython # To access the command-line interface docker run --rm -it -v $(pwd)/data:/data turicas/rows:0.4.0 rows print /data/test.csv","title":"Docker"},{"location":"installation/#installing-plugins","text":"The plugins csv , dicts , json , sqlite and txt are built-in by default but if you want to use another one you need to explicitly install its dependencies, for example: pip install rows[html] pip install rows[xls] Note: if you're running another command line interpreter (like zsh) you may need to escape the characters [ and ] . You also need to install some dependencies to use the command-line interface . You can do it installing the cli extra requirement: pip install rows[cli] And - easily - you can install all the dependencies by using the all extra requirement: pip install rows[all]","title":"Installing plugins"},{"location":"license/","text":"License This library is released under the GNU Lesser General Public License version 3 .","title":"License"},{"location":"license/#license","text":"This library is released under the GNU Lesser General Public License version 3 .","title":"License"},{"location":"links/","text":"Going deeper Showcase (Portuguese) How rows is helping make Brazilian data more accessible (Portuguese) Talk (videos + slides) on rows by \u00c1lvaro Justen Related and Similar Projects (Article) Data science at the command-line Ghost.py OKFN's goodtables OKFN's messytables Pipe Recorde TableFactory Tabula continuous-docs csvcat csvstudio dataconverters dateparser django-import-export extruct grablib import.io libextract libextract multicorn odo pandashells (and pandas DataFrame) parse proof records schema scrapelib scrapy screed selection streamtools table-extractor tablib telega-mega-import textql texttables validictory validr visidata webscraper.io Known Issues Create a better plugin interface so anyone can benefit of it Create an object to represent a set of rows.Table s, like TableSet Performance: the automatic type detection algorithm can cost time: it iterates over all rows to determine the type of each column. You can disable it by passing samples=0 to any import_from_* function or either changing the number of sample rows (any positive number is accepted). Code design issues","title":"Going deeper"},{"location":"links/#going-deeper","text":"","title":"Going deeper"},{"location":"links/#showcase","text":"(Portuguese) How rows is helping make Brazilian data more accessible (Portuguese) Talk (videos + slides) on rows by \u00c1lvaro Justen","title":"Showcase"},{"location":"links/#related-and-similar-projects","text":"(Article) Data science at the command-line Ghost.py OKFN's goodtables OKFN's messytables Pipe Recorde TableFactory Tabula continuous-docs csvcat csvstudio dataconverters dateparser django-import-export extruct grablib import.io libextract libextract multicorn odo pandashells (and pandas DataFrame) parse proof records schema scrapelib scrapy screed selection streamtools table-extractor tablib telega-mega-import textql texttables validictory validr visidata webscraper.io","title":"Related and Similar Projects"},{"location":"links/#known-issues","text":"Create a better plugin interface so anyone can benefit of it Create an object to represent a set of rows.Table s, like TableSet Performance: the automatic type detection algorithm can cost time: it iterates over all rows to determine the type of each column. You can disable it by passing samples=0 to any import_from_* function or either changing the number of sample rows (any positive number is accepted). Code design issues","title":"Known Issues"},{"location":"locale/","text":"Locale Many fields inside rows.fields are locale-aware. If you have some data using Brazilian Portuguese number formatting, for example ( , as decimal separators and . as thousands separator) you can configure this into the library and rows will automatically understand these numbers! Let's see it working by extracting the population of cities in Rio de Janeiro state: import locale import requests import rows from io import BytesIO url = \"http://cidades.ibge.gov.br/comparamun/compara.php?idtema=1&codv=v01&coduf=33\" html = requests.get(url).content with rows.locale_context(name=\"pt_BR.UTF-8\", category=locale.LC_NUMERIC): rio = rows.import_from_html(BytesIO(html)) total_population = sum(city.pessoas for city in rio) # 'pessoas' is the fieldname related to the number of people in each city print(f\"Rio de Janeiro has {total_population} inhabitants\") The column pessoas will be imported as an IntegerField and the result is: Rio de Janeiro has 15989929 inhabitants Locale dependency rows.locale_context depends on your operational system locales to work. In order to successfully use this context manager make sure the desired locale is available in a system level. For example, for Debian based systems: Make sure the desired locale is present and uncommented /etc/locale.gen Run locale-gen For more information see the code reference .","title":"Locale"},{"location":"locale/#locale","text":"Many fields inside rows.fields are locale-aware. If you have some data using Brazilian Portuguese number formatting, for example ( , as decimal separators and . as thousands separator) you can configure this into the library and rows will automatically understand these numbers! Let's see it working by extracting the population of cities in Rio de Janeiro state: import locale import requests import rows from io import BytesIO url = \"http://cidades.ibge.gov.br/comparamun/compara.php?idtema=1&codv=v01&coduf=33\" html = requests.get(url).content with rows.locale_context(name=\"pt_BR.UTF-8\", category=locale.LC_NUMERIC): rio = rows.import_from_html(BytesIO(html)) total_population = sum(city.pessoas for city in rio) # 'pessoas' is the fieldname related to the number of people in each city print(f\"Rio de Janeiro has {total_population} inhabitants\") The column pessoas will be imported as an IntegerField and the result is: Rio de Janeiro has 15989929 inhabitants","title":"Locale"},{"location":"locale/#locale-dependency","text":"rows.locale_context depends on your operational system locales to work. In order to successfully use this context manager make sure the desired locale is available in a system level. For example, for Debian based systems: Make sure the desired locale is present and uncommented /etc/locale.gen Run locale-gen For more information see the code reference .","title":"Locale dependency"},{"location":"operations/","text":"Table operations The module rows.operations contains some operations you can do on your Table objects: rows.operations.join : return a new Table based on the joining of a list of Table s and a field to act as key between them. Note: for performance reasons you may not use this function, since the join operation is done in Python - you can also convert everything to SQLite, query data there and then have your results in a Table , like the rows query command. rows.operations.transform : return a new Table based on other tables and a transformation function. rows.operations.transpose : transpose the Table based on a specific field. For more details see the reference .","title":"Operations"},{"location":"operations/#table-operations","text":"The module rows.operations contains some operations you can do on your Table objects: rows.operations.join : return a new Table based on the joining of a list of Table s and a field to act as key between them. Note: for performance reasons you may not use this function, since the join operation is done in Python - you can also convert everything to SQLite, query data there and then have your results in a Table , like the rows query command. rows.operations.transform : return a new Table based on other tables and a transformation function. rows.operations.transpose : transpose the Table based on a specific field. For more details see the reference .","title":"Table operations"},{"location":"plugins/","text":"Supported Plugins The idea behing plugins is very simple: it's a piece of code which extracts data from/exports to some specific format and interfaces with the core library functions, which will know how to detect and convert data types, export to other formats etc. If you don't find the plugin for the format you need, feel free to contribute . :-) Each import_from_X function receive specific parameters (depending on the format you're working) but also general parameters such as skip_header and fields (they are passed to the rows.plugins.utils.create_table function ). Some plugins also provide helper functions to work with the specific format, which can help a lot extracting non-tabular data (like rows.plugins.html.extract_links and rows.plugins.pdf.pdf_to_text ). This documentation is still in progress - please look into the plugins' source code to see all available parameters. Contributions on the documentation are very welcome. Look into the examples folder to see the plugins in action. :) Current implemented plugins: CSV List of dicts HTML JSON ODS Parquet PDF PostgreSQL SQLite TXT XLS XLSX XPath Note: rows is still not lazy by default, except for some operations like csv2sqlite , sqlite2csv , pgimport and pgexport (so using rows.import_from_X will put everything in memory), we're working on this . CSV See code reference Use rows.import_from_csv and rows.export_to_csv (dependencies are installed by default). The CSV dialect is detected automatically but you can specify it by passing the dialect parameter. Helper functions: rows.plugins.csv.discover_dialect : tries to figure out the CSV dialect based on a sample (in bytes). rows.utils.csv2sqlite : lazily convert a CSV into a SQLite table (the command-line version of this function is pretty useful -- see more by running rows csv2sqlite --help ). The CSV can be optionally compressed ( .csv , .csv.gz and .csv.xz ). Learn by example: examples/library/usa_legislators.py List of dicts See code reference Use rows.import_from_dicts and rows.export_to_dicts (no dependencies). Useful when you have the data in memory and would like to detect/convert data types and/or export to a supported format. Learn by example: examples/library/organizaciones.py HTML See code reference Use rows.import_from_html and rows.export_to_html (dependencies must be installed with pip install rows[html] ). You can specify the table index in case there's more than one <table> inside the HTML, decide whether to keep the HTML code inside the <td> tags (useful to extract links and \"hidden\" data) and other options. Very useful in Web scraping. Learn by example: examples/library/airports.py examples/library/extract_links.py examples/library/slip_opinions.py Helper functions: rows.plugins.html.count_tables : return the number of tables for a given HTML; rows.plugins.html.tag_to_dict : extract tag's attributes into a dict ; rows.plugins.html.extract_text : extract the text content from a given HTML; rows.plugins.html.extract_links : extract the href attributes from a given HTML (returns a list of strings). JSON See code reference Use rows.import_from_json and rows.export_to_json (no dependencies). Each table is converted to an array of objects (where each row is represented by an object). ODS See code reference Use rows.import_from_ods (dependencies must be installed with pip install rows[ods] ). Parquet See code reference Use rows.import_from_parquet passing the filename (dependencies must be installed with pip install rows[parquet] and if the data is compressed using snappy you'll also need to pip install rows[parquet-snappy] and the libsnappy-dev system library) -- read this blog post for more details and one example. PDF See code reference Use rows.import_from_pdf (dependencies must be installed with pip install rows[pdf] ). PDF Parser Backend There are two available backends (under-the-hood libraries to parse the PDF), which you can select by passing the backend parameter (results may differ depending on the backend): 'pymupdf' : use if possible, is much faster than the other option; 'pdfminer' : 100% Python implementation, very slow. Get this list programatically with rows.plugins.pdf.backends() . You can also subclass rows.plugins.pdf.PDFBackend and implement your own PDF parser, if needed. Specify Table Boundaries You can specify some parameters to delimit where the table is located in the PDF, like: starts_after and ends_before : delimits the objects before/after the table. Can be: regular strings (exact match); regular expressions objects; or functions (receives the object and must return True for the object which define if the table starts/ends there). page_numbers : sequence with desired page numbers (starts from 1 ). Specify Detection Algorithms There are 3 available algorithms to identify text objects and define where the table is located inside each page - you can subclass them and overwrite some methods to have custom behaviour (like the get_lines , where you can access objects' positions, for example). The algorithms available are (get the list programatically with rows.plugins.pdf.algorithms() ): rows.plugins.pdf.YGroupsAlgorithm : default, group text objects by y position and identify table lines based on these groups. rows.plugins.pdf.HeaderPositionAlgorithm : use the table header to identify cell positions and then fill the table with found objects (useful in sparse tables). rows.plugins.pdf.RectsBoundariesAlgorithm : detect the table boundaries by the rectangles on the page (currently only available using the 'pdfminer' backend, which is very slow). Helper Functions rows.plugins.pdf.number_of_pages : returns an integer representing the number of pages of a specific PDF file/stream; rows.plugins.pdf.pdf_to_text : generator: each iteration will return the text for a specific page (can specify page_numbers to delimit which pages will be returned); rows.plugins.pdf.pdf_table_lines : almost the same as rows.import_from_pdf , but returns a list of strings instead of a rows.Table object. Useful if the PDF is not well structured and needs some tweaking before importing as a rows.Table (so you can export to another format). Examples balneabilidade-brasil : downloads thousands of PDFs from Brazilian organizations which monitors water quality, then extract the tables in each PDF and put all rows together in one CSV; examples/cli/extract-pdf.sh : PDF extraction using the command-line interface (the parameters cannot be customized using this method by now -- more improvements in next versions). PostgreSQL See code reference Use rows.import_from_postgresql and rows.export_to_postgresql (dependencies must be installed with pip install rows[postgresql] ). Parameters On both rows.import_from_postgresql and rows.export_to_postgresql you can pass either a connection string or a psycopg2 connection object. On rows.import_from_postgresql you can pass a query parameter instead of a table_name . Helper Functions rows.utils.pgimport : import data from CSV into PostgreSQL using the fastest possible method - requires the psql command available on your system (the command-line version of this function is pretty useful -- see more by running rows pgimport --help ). The CSV can be optionally compressed ( .csv , .csv.gz and .csv.xz ); rows.utils.pgexport : export data from PostgreSQL into a CSV file using the fastest possible method - requires the psql command available on your system (the command-line version of this function is pretty useful -- see more by running rows pgexport --help ). The CSV can be optionally compressed ( .csv , .csv.gz and .csv.xz ). SQLite See code reference Use rows.import_from_sqlite and rows.export_to_sqlite (no dependencies). Helper functions: rows.utils.sqlite2csv : lazily SQLite tables into CSV files (the command-line version of this function is pretty useful -- see more by running rows sqlite2csv --help ). The CSV can be optionally compressed ( .csv , .csv.gz and .csv.xz ). TXT See code reference Use rows.import_from_txt and rows.export_to_txt (no dependencies). You can customize the border style. XLS See code reference Use rows.import_from_xls and rows.export_to_xls (dependencies must be installed with pip install rows[xls] ). You can customize things like sheet_name , sheet_index , start_row , end_row , start_column and end_column (the last 5 options are indexes and starts from 0). On rows.export_to_xls you can define the sheet_name . XLSX See code reference use rows.import_from_xlsx and rows.export_to_xlsx (dependencies must be installed with pip install rows[xlsx] ). You can customize things like sheet_name , sheet_index , start_row , end_row , start_column and end_column (the last 5 options are indexes and starts from 0). On rows.export_to_xlsx you can define the sheet_name . XPath See code reference Dependencies must be installed with pip install rows[xpath] ). Very useful in Web scraping. Use rows.import_from_xpath passing the following arguments: filename_or_fobj : source XML/HTML; rows_xpath : XPath to find the elements which will be transformed into rows; fields_xpath : collections.OrderedDict containing XPaths for each of the fields (key: field name, value: XPath string) - you'll probrably want to use ./ so it'll search inside the row found by rows_xpath ). Learn by example: examples/library/ecuador_radiodifusoras.py examples/library/brazilian_cities_wikipedia.py","title":"Plugins"},{"location":"plugins/#supported-plugins","text":"The idea behing plugins is very simple: it's a piece of code which extracts data from/exports to some specific format and interfaces with the core library functions, which will know how to detect and convert data types, export to other formats etc. If you don't find the plugin for the format you need, feel free to contribute . :-) Each import_from_X function receive specific parameters (depending on the format you're working) but also general parameters such as skip_header and fields (they are passed to the rows.plugins.utils.create_table function ). Some plugins also provide helper functions to work with the specific format, which can help a lot extracting non-tabular data (like rows.plugins.html.extract_links and rows.plugins.pdf.pdf_to_text ). This documentation is still in progress - please look into the plugins' source code to see all available parameters. Contributions on the documentation are very welcome. Look into the examples folder to see the plugins in action. :) Current implemented plugins: CSV List of dicts HTML JSON ODS Parquet PDF PostgreSQL SQLite TXT XLS XLSX XPath Note: rows is still not lazy by default, except for some operations like csv2sqlite , sqlite2csv , pgimport and pgexport (so using rows.import_from_X will put everything in memory), we're working on this .","title":"Supported Plugins"},{"location":"plugins/#csv","text":"See code reference Use rows.import_from_csv and rows.export_to_csv (dependencies are installed by default). The CSV dialect is detected automatically but you can specify it by passing the dialect parameter. Helper functions: rows.plugins.csv.discover_dialect : tries to figure out the CSV dialect based on a sample (in bytes). rows.utils.csv2sqlite : lazily convert a CSV into a SQLite table (the command-line version of this function is pretty useful -- see more by running rows csv2sqlite --help ). The CSV can be optionally compressed ( .csv , .csv.gz and .csv.xz ). Learn by example: examples/library/usa_legislators.py","title":"CSV"},{"location":"plugins/#list-of-dicts","text":"See code reference Use rows.import_from_dicts and rows.export_to_dicts (no dependencies). Useful when you have the data in memory and would like to detect/convert data types and/or export to a supported format. Learn by example: examples/library/organizaciones.py","title":"List of dicts"},{"location":"plugins/#html","text":"See code reference Use rows.import_from_html and rows.export_to_html (dependencies must be installed with pip install rows[html] ). You can specify the table index in case there's more than one <table> inside the HTML, decide whether to keep the HTML code inside the <td> tags (useful to extract links and \"hidden\" data) and other options. Very useful in Web scraping. Learn by example: examples/library/airports.py examples/library/extract_links.py examples/library/slip_opinions.py Helper functions: rows.plugins.html.count_tables : return the number of tables for a given HTML; rows.plugins.html.tag_to_dict : extract tag's attributes into a dict ; rows.plugins.html.extract_text : extract the text content from a given HTML; rows.plugins.html.extract_links : extract the href attributes from a given HTML (returns a list of strings).","title":"HTML"},{"location":"plugins/#json","text":"See code reference Use rows.import_from_json and rows.export_to_json (no dependencies). Each table is converted to an array of objects (where each row is represented by an object).","title":"JSON"},{"location":"plugins/#ods","text":"See code reference Use rows.import_from_ods (dependencies must be installed with pip install rows[ods] ).","title":"ODS"},{"location":"plugins/#parquet","text":"See code reference Use rows.import_from_parquet passing the filename (dependencies must be installed with pip install rows[parquet] and if the data is compressed using snappy you'll also need to pip install rows[parquet-snappy] and the libsnappy-dev system library) -- read this blog post for more details and one example.","title":"Parquet"},{"location":"plugins/#pdf","text":"See code reference Use rows.import_from_pdf (dependencies must be installed with pip install rows[pdf] ).","title":"PDF"},{"location":"plugins/#pdf-parser-backend","text":"There are two available backends (under-the-hood libraries to parse the PDF), which you can select by passing the backend parameter (results may differ depending on the backend): 'pymupdf' : use if possible, is much faster than the other option; 'pdfminer' : 100% Python implementation, very slow. Get this list programatically with rows.plugins.pdf.backends() . You can also subclass rows.plugins.pdf.PDFBackend and implement your own PDF parser, if needed.","title":"PDF Parser Backend"},{"location":"plugins/#specify-table-boundaries","text":"You can specify some parameters to delimit where the table is located in the PDF, like: starts_after and ends_before : delimits the objects before/after the table. Can be: regular strings (exact match); regular expressions objects; or functions (receives the object and must return True for the object which define if the table starts/ends there). page_numbers : sequence with desired page numbers (starts from 1 ).","title":"Specify Table Boundaries"},{"location":"plugins/#specify-detection-algorithms","text":"There are 3 available algorithms to identify text objects and define where the table is located inside each page - you can subclass them and overwrite some methods to have custom behaviour (like the get_lines , where you can access objects' positions, for example). The algorithms available are (get the list programatically with rows.plugins.pdf.algorithms() ): rows.plugins.pdf.YGroupsAlgorithm : default, group text objects by y position and identify table lines based on these groups. rows.plugins.pdf.HeaderPositionAlgorithm : use the table header to identify cell positions and then fill the table with found objects (useful in sparse tables). rows.plugins.pdf.RectsBoundariesAlgorithm : detect the table boundaries by the rectangles on the page (currently only available using the 'pdfminer' backend, which is very slow).","title":"Specify Detection Algorithms"},{"location":"plugins/#helper-functions","text":"rows.plugins.pdf.number_of_pages : returns an integer representing the number of pages of a specific PDF file/stream; rows.plugins.pdf.pdf_to_text : generator: each iteration will return the text for a specific page (can specify page_numbers to delimit which pages will be returned); rows.plugins.pdf.pdf_table_lines : almost the same as rows.import_from_pdf , but returns a list of strings instead of a rows.Table object. Useful if the PDF is not well structured and needs some tweaking before importing as a rows.Table (so you can export to another format).","title":"Helper Functions"},{"location":"plugins/#examples","text":"balneabilidade-brasil : downloads thousands of PDFs from Brazilian organizations which monitors water quality, then extract the tables in each PDF and put all rows together in one CSV; examples/cli/extract-pdf.sh : PDF extraction using the command-line interface (the parameters cannot be customized using this method by now -- more improvements in next versions).","title":"Examples"},{"location":"plugins/#postgresql","text":"See code reference Use rows.import_from_postgresql and rows.export_to_postgresql (dependencies must be installed with pip install rows[postgresql] ).","title":"PostgreSQL"},{"location":"plugins/#parameters","text":"On both rows.import_from_postgresql and rows.export_to_postgresql you can pass either a connection string or a psycopg2 connection object. On rows.import_from_postgresql you can pass a query parameter instead of a table_name .","title":"Parameters"},{"location":"plugins/#helper-functions_1","text":"rows.utils.pgimport : import data from CSV into PostgreSQL using the fastest possible method - requires the psql command available on your system (the command-line version of this function is pretty useful -- see more by running rows pgimport --help ). The CSV can be optionally compressed ( .csv , .csv.gz and .csv.xz ); rows.utils.pgexport : export data from PostgreSQL into a CSV file using the fastest possible method - requires the psql command available on your system (the command-line version of this function is pretty useful -- see more by running rows pgexport --help ). The CSV can be optionally compressed ( .csv , .csv.gz and .csv.xz ).","title":"Helper Functions"},{"location":"plugins/#sqlite","text":"See code reference Use rows.import_from_sqlite and rows.export_to_sqlite (no dependencies). Helper functions: rows.utils.sqlite2csv : lazily SQLite tables into CSV files (the command-line version of this function is pretty useful -- see more by running rows sqlite2csv --help ). The CSV can be optionally compressed ( .csv , .csv.gz and .csv.xz ).","title":"SQLite"},{"location":"plugins/#txt","text":"See code reference Use rows.import_from_txt and rows.export_to_txt (no dependencies). You can customize the border style.","title":"TXT"},{"location":"plugins/#xls","text":"See code reference Use rows.import_from_xls and rows.export_to_xls (dependencies must be installed with pip install rows[xls] ). You can customize things like sheet_name , sheet_index , start_row , end_row , start_column and end_column (the last 5 options are indexes and starts from 0). On rows.export_to_xls you can define the sheet_name .","title":"XLS"},{"location":"plugins/#xlsx","text":"See code reference use rows.import_from_xlsx and rows.export_to_xlsx (dependencies must be installed with pip install rows[xlsx] ). You can customize things like sheet_name , sheet_index , start_row , end_row , start_column and end_column (the last 5 options are indexes and starts from 0). On rows.export_to_xlsx you can define the sheet_name .","title":"XLSX"},{"location":"plugins/#xpath","text":"See code reference Dependencies must be installed with pip install rows[xpath] ). Very useful in Web scraping. Use rows.import_from_xpath passing the following arguments: filename_or_fobj : source XML/HTML; rows_xpath : XPath to find the elements which will be transformed into rows; fields_xpath : collections.OrderedDict containing XPaths for each of the fields (key: field name, value: XPath string) - you'll probrably want to use ./ so it'll search inside the row found by rows_xpath ). Learn by example: examples/library/ecuador_radiodifusoras.py examples/library/brazilian_cities_wikipedia.py","title":"XPath"},{"location":"quick-start/","text":"Quick Start Guide Programatically creating a Table object rows can import data from any of the supported formats (using rows.import_from_X functions) and will return a Table object for you, but you can also create a Table object by hand. Using Table.append from collections import OrderedDict from rows import fields, Table # Create a schema for the new table (check also all the available field types # inside `rows.fields`). country_fields = OrderedDict([ (\"name\", fields.TextField), (\"population\", fields.IntegerField), ]) # Data from: <http://www.worldometers.info/world-population/population-by-country/> countries = Table(fields=country_fields) countries.append({\"name\": \"Argentina\", \"population\": \"45101781\"}) countries.append({\"name\": \"Brazil\", \"population\": \"212392717\"}) countries.append({\"name\": \"Colombia\", \"population\": \"49849818\"}) countries.append({\"name\": \"Ecuador\", \"population\": \"17100444\"}) countries.append({\"name\": \"Peru\", \"population\": \"32933835\"}) Then you can iterate over it: for country in countries: print(country) # Result: # Row(name='Argentina', population=45101781) # Row(name='Brazil', population=212392717) # Row(name='Colombia', population=49849818) # Row(name='Ecuador', population=17100444) # Row(name='Peru', population=32933835) # \"Row\" is a namedtuple created from `country_fields` # We've added population as a string, the library automatically converted to # integer so we can also sum: countries_population = sum(country.population for country in countries) print(countries_population) # prints 357378595 You could also export this table to CSV or any other supported format: import rows rows.export_to_csv(countries, \"some-LA-countries.csv\") If you had this file before, you could: import rows countries = rows.import_from_csv(\"some-LA-countries.csv\") for country in countries: print(country) # And the result will be the same. # Since the library has an automatic type detector, the \"population\" column # will be detected and converted to integer. Let's see the detected types: print(countries.fields) # Result: # OrderedDict([ # ('name', <class 'rows.fields.TextField'>), # ('population', <class 'rows.fields.IntegerField'>) # ]) From a list of dict s If you have the data in a list of dictionaries already you can simply use rows.import_from_dicts : import rows data = [ {\"name\": \"Argentina\", \"population\": \"45101781\"}, {\"name\": \"Brazil\", \"population\": \"212392717\"}, {\"name\": \"Colombia\", \"population\": \"49849818\"}, {\"name\": \"Ecuador\", \"population\": \"17100444\"}, {\"name\": \"Peru\", \"population\": \"32933835\"}, {\"name\": \"Guyana\", }, # Missing \"population\", will fill with `None` ] table = rows.import_from_dicts(data) print(table[-1]) # Can use indexes # Result: # Row(name='Guyana', population=None) Importing from other formats rows ' ability to import data is amazing: its plugins will do the hard job of parsing the file format so you don't need to. They can help you exporting data also. For example, let's download a CSV from the Web and import it: import requests import rows from io import BytesIO url = \"http://unitedstates.sunlightfoundation.com/legislators/legislators.csv\" csv = requests.get(url).content # Download CSV data legislators = rows.import_from_csv(BytesIO(csv)) # already imported! print(\"rows automatically identified the types:\") for field_name, field_type in legislators.fields.items(): print(f\"{field_name} is {field_type}\") And you'll see something like this: [...] gender is <class 'rows.fields.TextField'> [...] govtrack_id is <class 'rows.fields.IntegerField'> [...] birthdate is <class 'rows.fields.DateField'> [...] Note that native Python objects are returned for each row inside a namedtuple ! The library recognizes each field type and converts it automagically no matter which plugin you're using to import the data. We can then work on this data: women = sum(1 for row in legislators if row.in_office and row.gender == 'F') men = sum(1 for row in legislators if row.in_office and row.gender == 'M') print(f\"Women vs Men (in office): {women} vs {men}.\") # Result: # Women vs Men: 108 vs 432. Since birthdate is automatically detected and converted to a rows.fields.DateField we can do some quick analysis: legislators.order_by(\"birthdate\") older, younger = legislators[-1], legislators[0] print(f\"{older.lastname}, {older.firstname} is older than {younger.lastname}, {younger.firstname}.\") # Result: # Stefanik, Elise is older than Byrd, Robert. You can also get a whole column, like this: print(legislators[\"gender\"]) # Result (a list of strings): # ['M', 'M', 'M', 'M', 'M', 'M', ..., 'M', 'M', 'F'] And change the whole column (or add a new one): legislators[\"gender\"] = [ \"male\" if gender == \"M\" else \"female\" for gender in legislators[\"gender\"] ] print(legislators[\"gender\"]) # Result: # ['male', 'male', 'male', ..., 'male', 'female'] Or delete it: print(\"gender\" in legislators.field_names) # Result: True del legislators[\"gender\"] print(\"gender\" in legislators.field_names) # Result: False print(legislators[0].gender) # Raises the exception: # AttributeError: 'Row' object has no attribute 'gender' Exercise: use rows.import_from_html to import population data from worldometers.com (tip: you must run pip install rows[html] first to install the needed dependencies). Common Parameters Each plugin has its own parameters (like index in import_from_html and sheet_name in import_from_xls ) but all plugins create a rows.Table object so they also have some common parameters you can pass to import_from_X . They are: fields : an OrderedDict with field names and types (disable automatic detection of types). force_types : a dict mapping field names to field types you'd like to force, so rows won't try to detect it. Example: {\"population\": rows.fields.IntegerField} . skip_header : Ignore header row. Only used if fields is not None . Default: True . import_fields : a list with field names to import (other fields will be ignored) -- fields will be imported in this order. export_fields : a list with field names to export (other fields will be ignored) -- fields will be exported in this order. samples : number of sample rows to use on field type autodetect algorithm. Default: None (use all rows). Exporting Data If you have a Table object you can export it to all available plugins which have the \"export\" feature. Let's use the HTML plugin: rows.export_to_html(legislators, \"legislators.html\") And you'll get a file with the following contents: <table> <thead> <tr> <th> title </th> <th> firstname </th> <th> middlename </th> <th> lastname </th> <th> name_suffix </th> <th> nickname </th> [...] </tbody> </table> Exporting to memory Some plugins don't require a filename to export to, so you can get the result as a string, for example: fields_to_export = (\"title\", \"firstname\", \"lastname\", \"party\") content = rows.export_to_txt(legislators, export_fields=fields_to_export) print(content) The result will be: +-------+-------------+--------------------+-------+ | title | firstname | lastname | party | +-------+-------------+--------------------+-------+ | Sen | Robert | Byrd | D | | Rep | Ralph | Hall | R | | Sen | Ted | Stevens | R | | Sen | Frank | Lautenberg | D | [...] | Rep | Aaron | Schock | R | | Rep | Matt | Gaetz | R | | Rep | Trey | Hollingsworth | R | | Rep | Mike | Gallagher | R | | Rep | Elise | Stefanik | R | +-------+-------------+--------------------+-------+ The plugins csv , json and html have this behaviour. It makes sense on file-oriented formats to returned the data as output, but some plugins return different objects; on sqlite the returned object is a sqlite3.Connection , see: connection = rows.export_to_sqlite(legislators, \":memory:\") query = \"SELECT firstname, lastname FROM table1 WHERE birthdate > 1980-01-01\" connection = rows.export_to_sqlite(legislators, \":memory:\") print(list(connection.execute(query).fetchall())) You'll get the following output: [('Darren', 'Soto'), ('Adam', 'Kinzinger'), ('Ron', 'DeSantis'), (...)] Using file and connection objects The majority of plugins also accept file-objects instead of filenames (for importing and also for exporting), for example: from io import BytesIO fobj = BytesIO() rows.export_to_csv(legislators, fobj) fobj.seek(0) # You need to point the file cursor to the first position. print(fobj.read()) The following text will be printed: b\"title,firstname,lastname,party\\r\\nSen,Robert,Byrd,D\\r\\nRep,Ralph,Hall,R[...]\" The same happens for sqlite3.Connection objects when importing: # Reuses the `connection` and `query` variables from the last sections' example table = rows.import_from_sqlite(connection, query=query) print(rows.export_to_txt(table)) The following output will be printed: +-----------+-----------------+ | firstname | lastname | +-----------+-----------------+ | Darren | Soto | | Adam | Kinzinger | | Ron | DeSantis | | Stephanie | Murphy | | Seth | Moulton | | Jaime | Herrera Beutler | | Pete | Aguilar | | Scott | Taylor | | Jim | Banks | | Ruben | Gallego | | Lee | Zeldin | | Carlos | Curbelo | | Justin | Amash | | Ruben | Kihuen | | Jason | Smith | | Brian | Mast | | Joseph | Kennedy | | Eric | Swalwell | | Tulsi | Gabbard | | Aaron | Schock | | Matt | Gaetz | | Trey | Hollingsworth | | Mike | Gallagher | | Elise | Stefanik | +-----------+-----------------+ Learn more Now you have finished the quickstart guide. See the examples folder for more examples.","title":"Quick Start Guide"},{"location":"quick-start/#quick-start-guide","text":"","title":"Quick Start Guide"},{"location":"quick-start/#programatically-creating-a-table-object","text":"rows can import data from any of the supported formats (using rows.import_from_X functions) and will return a Table object for you, but you can also create a Table object by hand.","title":"Programatically creating a Table object"},{"location":"quick-start/#using-tableappend","text":"from collections import OrderedDict from rows import fields, Table # Create a schema for the new table (check also all the available field types # inside `rows.fields`). country_fields = OrderedDict([ (\"name\", fields.TextField), (\"population\", fields.IntegerField), ]) # Data from: <http://www.worldometers.info/world-population/population-by-country/> countries = Table(fields=country_fields) countries.append({\"name\": \"Argentina\", \"population\": \"45101781\"}) countries.append({\"name\": \"Brazil\", \"population\": \"212392717\"}) countries.append({\"name\": \"Colombia\", \"population\": \"49849818\"}) countries.append({\"name\": \"Ecuador\", \"population\": \"17100444\"}) countries.append({\"name\": \"Peru\", \"population\": \"32933835\"}) Then you can iterate over it: for country in countries: print(country) # Result: # Row(name='Argentina', population=45101781) # Row(name='Brazil', population=212392717) # Row(name='Colombia', population=49849818) # Row(name='Ecuador', population=17100444) # Row(name='Peru', population=32933835) # \"Row\" is a namedtuple created from `country_fields` # We've added population as a string, the library automatically converted to # integer so we can also sum: countries_population = sum(country.population for country in countries) print(countries_population) # prints 357378595 You could also export this table to CSV or any other supported format: import rows rows.export_to_csv(countries, \"some-LA-countries.csv\") If you had this file before, you could: import rows countries = rows.import_from_csv(\"some-LA-countries.csv\") for country in countries: print(country) # And the result will be the same. # Since the library has an automatic type detector, the \"population\" column # will be detected and converted to integer. Let's see the detected types: print(countries.fields) # Result: # OrderedDict([ # ('name', <class 'rows.fields.TextField'>), # ('population', <class 'rows.fields.IntegerField'>) # ])","title":"Using Table.append"},{"location":"quick-start/#from-a-list-of-dicts","text":"If you have the data in a list of dictionaries already you can simply use rows.import_from_dicts : import rows data = [ {\"name\": \"Argentina\", \"population\": \"45101781\"}, {\"name\": \"Brazil\", \"population\": \"212392717\"}, {\"name\": \"Colombia\", \"population\": \"49849818\"}, {\"name\": \"Ecuador\", \"population\": \"17100444\"}, {\"name\": \"Peru\", \"population\": \"32933835\"}, {\"name\": \"Guyana\", }, # Missing \"population\", will fill with `None` ] table = rows.import_from_dicts(data) print(table[-1]) # Can use indexes # Result: # Row(name='Guyana', population=None)","title":"From a list of dicts"},{"location":"quick-start/#importing-from-other-formats","text":"rows ' ability to import data is amazing: its plugins will do the hard job of parsing the file format so you don't need to. They can help you exporting data also. For example, let's download a CSV from the Web and import it: import requests import rows from io import BytesIO url = \"http://unitedstates.sunlightfoundation.com/legislators/legislators.csv\" csv = requests.get(url).content # Download CSV data legislators = rows.import_from_csv(BytesIO(csv)) # already imported! print(\"rows automatically identified the types:\") for field_name, field_type in legislators.fields.items(): print(f\"{field_name} is {field_type}\") And you'll see something like this: [...] gender is <class 'rows.fields.TextField'> [...] govtrack_id is <class 'rows.fields.IntegerField'> [...] birthdate is <class 'rows.fields.DateField'> [...] Note that native Python objects are returned for each row inside a namedtuple ! The library recognizes each field type and converts it automagically no matter which plugin you're using to import the data. We can then work on this data: women = sum(1 for row in legislators if row.in_office and row.gender == 'F') men = sum(1 for row in legislators if row.in_office and row.gender == 'M') print(f\"Women vs Men (in office): {women} vs {men}.\") # Result: # Women vs Men: 108 vs 432. Since birthdate is automatically detected and converted to a rows.fields.DateField we can do some quick analysis: legislators.order_by(\"birthdate\") older, younger = legislators[-1], legislators[0] print(f\"{older.lastname}, {older.firstname} is older than {younger.lastname}, {younger.firstname}.\") # Result: # Stefanik, Elise is older than Byrd, Robert. You can also get a whole column, like this: print(legislators[\"gender\"]) # Result (a list of strings): # ['M', 'M', 'M', 'M', 'M', 'M', ..., 'M', 'M', 'F'] And change the whole column (or add a new one): legislators[\"gender\"] = [ \"male\" if gender == \"M\" else \"female\" for gender in legislators[\"gender\"] ] print(legislators[\"gender\"]) # Result: # ['male', 'male', 'male', ..., 'male', 'female'] Or delete it: print(\"gender\" in legislators.field_names) # Result: True del legislators[\"gender\"] print(\"gender\" in legislators.field_names) # Result: False print(legislators[0].gender) # Raises the exception: # AttributeError: 'Row' object has no attribute 'gender' Exercise: use rows.import_from_html to import population data from worldometers.com (tip: you must run pip install rows[html] first to install the needed dependencies).","title":"Importing from other formats"},{"location":"quick-start/#common-parameters","text":"Each plugin has its own parameters (like index in import_from_html and sheet_name in import_from_xls ) but all plugins create a rows.Table object so they also have some common parameters you can pass to import_from_X . They are: fields : an OrderedDict with field names and types (disable automatic detection of types). force_types : a dict mapping field names to field types you'd like to force, so rows won't try to detect it. Example: {\"population\": rows.fields.IntegerField} . skip_header : Ignore header row. Only used if fields is not None . Default: True . import_fields : a list with field names to import (other fields will be ignored) -- fields will be imported in this order. export_fields : a list with field names to export (other fields will be ignored) -- fields will be exported in this order. samples : number of sample rows to use on field type autodetect algorithm. Default: None (use all rows).","title":"Common Parameters"},{"location":"quick-start/#exporting-data","text":"If you have a Table object you can export it to all available plugins which have the \"export\" feature. Let's use the HTML plugin: rows.export_to_html(legislators, \"legislators.html\") And you'll get a file with the following contents: <table> <thead> <tr> <th> title </th> <th> firstname </th> <th> middlename </th> <th> lastname </th> <th> name_suffix </th> <th> nickname </th> [...] </tbody> </table>","title":"Exporting Data"},{"location":"quick-start/#exporting-to-memory","text":"Some plugins don't require a filename to export to, so you can get the result as a string, for example: fields_to_export = (\"title\", \"firstname\", \"lastname\", \"party\") content = rows.export_to_txt(legislators, export_fields=fields_to_export) print(content) The result will be: +-------+-------------+--------------------+-------+ | title | firstname | lastname | party | +-------+-------------+--------------------+-------+ | Sen | Robert | Byrd | D | | Rep | Ralph | Hall | R | | Sen | Ted | Stevens | R | | Sen | Frank | Lautenberg | D | [...] | Rep | Aaron | Schock | R | | Rep | Matt | Gaetz | R | | Rep | Trey | Hollingsworth | R | | Rep | Mike | Gallagher | R | | Rep | Elise | Stefanik | R | +-------+-------------+--------------------+-------+ The plugins csv , json and html have this behaviour. It makes sense on file-oriented formats to returned the data as output, but some plugins return different objects; on sqlite the returned object is a sqlite3.Connection , see: connection = rows.export_to_sqlite(legislators, \":memory:\") query = \"SELECT firstname, lastname FROM table1 WHERE birthdate > 1980-01-01\" connection = rows.export_to_sqlite(legislators, \":memory:\") print(list(connection.execute(query).fetchall())) You'll get the following output: [('Darren', 'Soto'), ('Adam', 'Kinzinger'), ('Ron', 'DeSantis'), (...)]","title":"Exporting to memory"},{"location":"quick-start/#using-file-and-connection-objects","text":"The majority of plugins also accept file-objects instead of filenames (for importing and also for exporting), for example: from io import BytesIO fobj = BytesIO() rows.export_to_csv(legislators, fobj) fobj.seek(0) # You need to point the file cursor to the first position. print(fobj.read()) The following text will be printed: b\"title,firstname,lastname,party\\r\\nSen,Robert,Byrd,D\\r\\nRep,Ralph,Hall,R[...]\" The same happens for sqlite3.Connection objects when importing: # Reuses the `connection` and `query` variables from the last sections' example table = rows.import_from_sqlite(connection, query=query) print(rows.export_to_txt(table)) The following output will be printed: +-----------+-----------------+ | firstname | lastname | +-----------+-----------------+ | Darren | Soto | | Adam | Kinzinger | | Ron | DeSantis | | Stephanie | Murphy | | Seth | Moulton | | Jaime | Herrera Beutler | | Pete | Aguilar | | Scott | Taylor | | Jim | Banks | | Ruben | Gallego | | Lee | Zeldin | | Carlos | Curbelo | | Justin | Amash | | Ruben | Kihuen | | Jason | Smith | | Brian | Mast | | Joseph | Kennedy | | Eric | Swalwell | | Tulsi | Gabbard | | Aaron | Schock | | Matt | Gaetz | | Trey | Hollingsworth | | Mike | Gallagher | | Elise | Stefanik | +-----------+-----------------+","title":"Using file and connection objects"},{"location":"quick-start/#learn-more","text":"Now you have finished the quickstart guide. See the examples folder for more examples.","title":"Learn more"}]}